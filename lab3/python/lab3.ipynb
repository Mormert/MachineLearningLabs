{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Machine Learning </center>\n",
    "## <center> Lab 3 </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises for Pass (G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Neural Network implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uYF6zY4QTqY5",
    "outputId": "38840d62-8fe5-4ea9-d0e7-03e3ffb0f15b",
    "ExecuteTime": {
     "end_time": "2023-11-25T11:52:58.320853Z",
     "start_time": "2023-11-25T11:52:58.095388Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 100) (1, 100)\n",
      "[(5, 8), (10, 5), (5, 1), (10, 1)]\n",
      "3.707269632934111e-10\n"
     ]
    }
   ],
   "source": [
    "# Time to implement a 1-layered non-regularized neural network.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_gradient(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "def check_gradient(J, theta):\n",
    "    numgrad = np.zeros_like(theta)\n",
    "    epsilon = 1e-4\n",
    "\n",
    "    for i in range(len(numgrad)):\n",
    "        e = np.zeros_like(theta).flatten()\n",
    "        e[i] = epsilon\n",
    "        numgrad[i] = (J(theta + e.reshape(theta.shape)) - J(theta - e.reshape(theta.shape))) / (2 * epsilon)\n",
    "\n",
    "    return numgrad.flatten()\n",
    "\n",
    "def init_nn_parameters(numin, numhid, numout):\n",
    "    r1  = np.sqrt(6) / np.sqrt(numin+numhid+1)\n",
    "    r2  = np.sqrt(6) / np.sqrt(numin+numout+1)\n",
    "    W1 = np.random.rand(numhid, numin) * 2 * r1 - r1\n",
    "    W2 = np.random.rand(numout, numhid) * 2 * r2 - r2\n",
    "    b1 = np.zeros((numhid, 1))\n",
    "    b2 = np.zeros((numout, 1))\n",
    "    theta = np.concatenate([W1.ravel(), W2.ravel(), b1.ravel(), b2.ravel()])\n",
    "    theta_size = [W1.shape, W2.shape, b1.shape, b2.shape]\n",
    "    return theta, theta_size\n",
    "\n",
    "def theta_to_params(theta, theta_size):\n",
    "    idx_W1_end = np.prod(theta_size[0])\n",
    "    idx_W2_end = idx_W1_end + np.prod(theta_size[1])\n",
    "    idx_b1_end = idx_W2_end + np.prod(theta_size[2])\n",
    "    W1 = theta[:idx_W1_end].reshape(theta_size[0])\n",
    "    W2 = theta[idx_W1_end:idx_W2_end].reshape(theta_size[1])\n",
    "    b1 = theta[idx_W2_end:idx_b1_end].reshape(theta_size[2])\n",
    "    b2 = theta[idx_b1_end:].reshape(theta_size[3])\n",
    "    return W1, W2, b1, b2\n",
    "\n",
    "def cost_neural_network(theta, theta_size, X, y):\n",
    "    m, n = X.shape\n",
    "    num_classes = len(np.unique(y))\n",
    "    lambda_ = 0\n",
    "\n",
    "    W1, W2, b1, b2 = theta_to_params(theta, theta_size) \n",
    "    y_mat = np.eye(num_classes)[y.reshape(-1)-1].T\n",
    "    \n",
    "    # =============== Your code here ============================\n",
    "    # Forward propagation\n",
    "    z2 = np.dot(W1, X) + b1\n",
    "    a2 = sigmoid(z2)\n",
    "    z3 = np.dot(W2, a2) + b2\n",
    "    a3 = sigmoid(z3)\n",
    "\n",
    "    # Calculate cost J\n",
    "    J = np.sum(0.5 * np.sum((a3 - y_mat) ** 2))\n",
    "\n",
    "    # Calculate delta error terms\n",
    "    delta3 = (a3 - y_mat) * a3 * (1 - a3)\n",
    "    delta2 = np.dot(W2.T, delta3) * a2 * (1 - a2)\n",
    "\n",
    "    # Calculate gradients\n",
    "    gradW2 = np.dot(delta3, a2.T)\n",
    "    gradW1 = np.dot(delta2, X.T)\n",
    "    gradb2 = np.sum(delta3, axis=1)\n",
    "    gradb1 = np.sum(delta2, axis=1)\n",
    "\n",
    "     # =============================================================\n",
    "    \n",
    "    # Unroll gradients\n",
    "    grad = np.concatenate([gradW1.ravel(), gradW2.ravel(), gradb1.ravel(), gradb2.ravel()])\n",
    "    \n",
    "    return J, grad\n",
    "\n",
    "# Create test data to check gradients\n",
    "X = np.random.randn(8, 100)\n",
    "y = np.random.randint(1, 11, size=(1, 100))\n",
    "print(X.shape,y.shape)\n",
    "\n",
    "theta, theta_size = init_nn_parameters(8, 5, 10)\n",
    "print(theta_size)\n",
    "cost, grad = cost_neural_network(theta, theta_size, X, y[0])  # Adjust indexing here since y[0] gives the 1D array\n",
    "numGrad = check_gradient(lambda p: cost_neural_network(p, theta_size, X, y[0])[0], theta)  # Adjust indexing here too\n",
    "diff = np.linalg.norm(numGrad - grad) / np.linalg.norm(numGrad + grad)\n",
    "print(diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Neural network for handwritten digit classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ofFQfRvVdMV4",
    "outputId": "71b0bf70-621f-4809-c009-6a7371357f64",
    "ExecuteTime": {
     "end_time": "2023-11-25T11:53:05.871128Z",
     "start_time": "2023-11-25T11:53:02.855510Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 400)\n",
      "[ 1  2  3  4  5  6  7  8  9 10]\n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =        20560     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  3.57057D+03    |proj g|=  3.38764D+02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate    1    f=  1.60593D+03    |proj g|=  8.11629D+01\n",
      "\n",
      "At iterate    2    f=  1.40476D+03    |proj g|=  2.69524D+01\n",
      "\n",
      "At iterate    3    f=  1.35700D+03    |proj g|=  9.42081D+00\n",
      "\n",
      "At iterate    4    f=  1.34608D+03    |proj g|=  4.45356D+00\n",
      "\n",
      "At iterate    5    f=  1.34226D+03    |proj g|=  3.92513D+00\n",
      "\n",
      "At iterate    6    f=  1.33704D+03    |proj g|=  4.75438D+00\n",
      "\n",
      "At iterate    7    f=  1.32050D+03    |proj g|=  9.87330D+00\n",
      "\n",
      "At iterate    8    f=  1.20963D+03    |proj g|=  2.00337D+01\n",
      "\n",
      "At iterate    9    f=  1.18295D+03    |proj g|=  5.56320D+01\n",
      "\n",
      "At iterate   10    f=  9.36538D+02    |proj g|=  5.59308D+01\n",
      "\n",
      "At iterate   11    f=  8.12597D+02    |proj g|=  2.13916D+01\n",
      "\n",
      "At iterate   12    f=  7.83722D+02    |proj g|=  1.86567D+01\n",
      "\n",
      "At iterate   13    f=  7.59697D+02    |proj g|=  2.08649D+01\n",
      "\n",
      "At iterate   14    f=  7.17880D+02    |proj g|=  1.75096D+01\n",
      "\n",
      "At iterate   15    f=  6.68016D+02    |proj g|=  3.05300D+01\n",
      "\n",
      "At iterate   16    f=  6.23556D+02    |proj g|=  1.22986D+01\n",
      "\n",
      "At iterate   17    f=  6.00337D+02    |proj g|=  1.03523D+01\n",
      "\n",
      "At iterate   18    f=  5.81641D+02    |proj g|=  6.88754D+00\n",
      "\n",
      "At iterate   19    f=  5.52081D+02    |proj g|=  8.11250D+00\n",
      "\n",
      "At iterate   20    f=  5.24672D+02    |proj g|=  1.35420D+01\n",
      "\n",
      "At iterate   21    f=  5.09111D+02    |proj g|=  4.91029D+00\n",
      "\n",
      "At iterate   22    f=  4.93554D+02    |proj g|=  2.61168D+00\n",
      "\n",
      "At iterate   23    f=  4.82474D+02    |proj g|=  3.83523D+00\n",
      "\n",
      "At iterate   24    f=  4.63262D+02    |proj g|=  4.09563D+00\n",
      "\n",
      "At iterate   25    f=  4.52224D+02    |proj g|=  4.31896D+00\n",
      "\n",
      "At iterate   26    f=  4.40539D+02    |proj g|=  1.73834D+00\n",
      "\n",
      "At iterate   27    f=  4.32053D+02    |proj g|=  1.40314D+00\n",
      "\n",
      "At iterate   28    f=  4.19496D+02    |proj g|=  2.22465D+00\n",
      "\n",
      "At iterate   29    f=  4.07629D+02    |proj g|=  2.83112D+00\n",
      "\n",
      "At iterate   30    f=  3.95861D+02    |proj g|=  2.10014D+00\n",
      "\n",
      "At iterate   31    f=  3.87137D+02    |proj g|=  3.25013D+00\n",
      "\n",
      "At iterate   32    f=  3.78821D+02    |proj g|=  8.63853D+00\n",
      "\n",
      "At iterate   33    f=  3.73144D+02    |proj g|=  1.38104D+01\n",
      "\n",
      "At iterate   34    f=  3.61488D+02    |proj g|=  2.10335D+01\n",
      "\n",
      "At iterate   35    f=  3.45164D+02    |proj g|=  8.45595D+00\n",
      "\n",
      "At iterate   36    f=  3.31365D+02    |proj g|=  7.48800D+00\n",
      "\n",
      "At iterate   37    f=  3.09742D+02    |proj g|=  9.97973D+00\n",
      "\n",
      "At iterate   38    f=  2.94812D+02    |proj g|=  4.45501D+00\n",
      "\n",
      "At iterate   39    f=  2.84411D+02    |proj g|=  3.94099D+00\n",
      "\n",
      "At iterate   40    f=  2.71540D+02    |proj g|=  2.81260D+00\n",
      "\n",
      "At iterate   41    f=  2.62615D+02    |proj g|=  3.11830D+00\n",
      "\n",
      "At iterate   42    f=  2.56584D+02    |proj g|=  1.90458D+00\n",
      "\n",
      "At iterate   43    f=  2.50668D+02    |proj g|=  1.65456D+00\n",
      "\n",
      "At iterate   44    f=  2.45286D+02    |proj g|=  1.42062D+00\n",
      "\n",
      "At iterate   45    f=  2.39588D+02    |proj g|=  1.54794D+00\n",
      "\n",
      "At iterate   46    f=  2.34389D+02    |proj g|=  1.26374D+00\n",
      "\n",
      "At iterate   47    f=  2.30284D+02    |proj g|=  1.29158D+00\n",
      "\n",
      "At iterate   48    f=  2.24994D+02    |proj g|=  1.31143D+00\n",
      "\n",
      "At iterate   49    f=  2.22987D+02    |proj g|=  2.87970D+00\n",
      "\n",
      "At iterate   50    f=  2.19898D+02    |proj g|=  8.87268D-01\n",
      "\n",
      "At iterate   51    f=  2.17611D+02    |proj g|=  9.98606D-01\n",
      "\n",
      "At iterate   52    f=  2.14671D+02    |proj g|=  1.17812D+00\n",
      "\n",
      "At iterate   53    f=  2.11897D+02    |proj g|=  9.48199D-01\n",
      "\n",
      "At iterate   54    f=  2.09030D+02    |proj g|=  5.28972D-01\n",
      "\n",
      "At iterate   55    f=  2.07601D+02    |proj g|=  3.80743D-01\n",
      "\n",
      "At iterate   56    f=  2.05708D+02    |proj g|=  5.43436D-01\n",
      "\n",
      "At iterate   57    f=  2.04197D+02    |proj g|=  5.06207D-01\n",
      "\n",
      "At iterate   58    f=  2.02457D+02    |proj g|=  5.46517D-01\n",
      "\n",
      "At iterate   59    f=  2.01045D+02    |proj g|=  2.19091D-01\n",
      "\n",
      "At iterate   60    f=  2.00085D+02    |proj g|=  2.13076D-01\n",
      "\n",
      "At iterate   61    f=  1.99125D+02    |proj g|=  4.79737D-01\n",
      "\n",
      "At iterate   62    f=  1.98159D+02    |proj g|=  4.45945D-01\n",
      "\n",
      "At iterate   63    f=  1.97328D+02    |proj g|=  2.21376D-01\n",
      "\n",
      "At iterate   64    f=  1.96588D+02    |proj g|=  1.94986D-01\n",
      "\n",
      "At iterate   65    f=  1.95951D+02    |proj g|=  2.36724D-01\n",
      "\n",
      "At iterate   66    f=  1.95510D+02    |proj g|=  1.69900D-01\n",
      "\n",
      "At iterate   67    f=  1.95207D+02    |proj g|=  1.15627D-01\n",
      "\n",
      "At iterate   68    f=  1.94517D+02    |proj g|=  1.57817D-01\n",
      "\n",
      "At iterate   69    f=  1.94267D+02    |proj g|=  1.99687D-01\n",
      "\n",
      "At iterate   70    f=  1.93878D+02    |proj g|=  1.32621D-01\n",
      "\n",
      "At iterate   71    f=  1.93087D+02    |proj g|=  2.68176D-01\n",
      "\n",
      "At iterate   72    f=  1.92552D+02    |proj g|=  2.40668D-01\n",
      "\n",
      "At iterate   73    f=  1.92022D+02    |proj g|=  2.02847D-01\n",
      "\n",
      "At iterate   74    f=  1.91505D+02    |proj g|=  1.40319D-01\n",
      "\n",
      "At iterate   75    f=  1.91292D+02    |proj g|=  1.99158D-01\n",
      "\n",
      "At iterate   76    f=  1.91126D+02    |proj g|=  1.06281D-01\n",
      "\n",
      "At iterate   77    f=  1.90926D+02    |proj g|=  1.39570D-01\n",
      "\n",
      "At iterate   78    f=  1.90546D+02    |proj g|=  1.38210D-01\n",
      "\n",
      "At iterate   79    f=  1.90434D+02    |proj g|=  1.36812D-01\n",
      "\n",
      "At iterate   80    f=  1.90211D+02    |proj g|=  2.32510D-01\n",
      "\n",
      "At iterate   81    f=  1.90126D+02    |proj g|=  1.74900D-01\n",
      "\n",
      "At iterate   82    f=  1.89953D+02    |proj g|=  1.19875D-01\n",
      "\n",
      "At iterate   83    f=  1.89786D+02    |proj g|=  6.50234D-02\n",
      "\n",
      "At iterate   84    f=  1.89659D+02    |proj g|=  4.39573D-02\n",
      "\n",
      "At iterate   85    f=  1.88053D+02    |proj g|=  1.97503D-01\n",
      "\n",
      "At iterate   86    f=  1.87858D+02    |proj g|=  3.51630D-01\n",
      "\n",
      "At iterate   87    f=  1.87513D+02    |proj g|=  4.00031D-01\n",
      "\n",
      "At iterate   88    f=  1.86557D+02    |proj g|=  2.51442D-01\n",
      "\n",
      "At iterate   89    f=  1.86259D+02    |proj g|=  1.76556D-01\n",
      "\n",
      "At iterate   90    f=  1.86083D+02    |proj g|=  2.93615D-01\n",
      "\n",
      "At iterate   91    f=  1.85881D+02    |proj g|=  8.09001D-02\n",
      "\n",
      "At iterate   92    f=  1.85843D+02    |proj g|=  7.38242D-02\n",
      "\n",
      "At iterate   93    f=  1.85705D+02    |proj g|=  4.28385D-02\n",
      "\n",
      "At iterate   94    f=  1.85620D+02    |proj g|=  2.61562D-02\n",
      "\n",
      "At iterate   95    f=  1.85129D+02    |proj g|=  4.61502D-02\n",
      "\n",
      "At iterate   96    f=  1.85105D+02    |proj g|=  3.43949D-02\n",
      "\n",
      "At iterate   97    f=  1.85065D+02    |proj g|=  2.04573D-02\n",
      "\n",
      "At iterate   98    f=  1.85052D+02    |proj g|=  6.21222D-03\n",
      "\n",
      "At iterate   99    f=  1.85043D+02    |proj g|=  8.26961D-03\n",
      "\n",
      "At iterate  100    f=  1.84656D+02    |proj g|=  5.23617D-02\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "20560    100    122      1     0     0   5.236D-02   1.847D+02\n",
      "  F =   184.65639386231879     \n",
      "\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 \n",
      "Train Set Accuracy: 88.13333333333333\n",
      "Val Set Accuracy: 83.13333333333334\n",
      "Test Set Accuracy: 85.8\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from scipy.optimize import minimize\n",
    "import scipy.io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def theta_to_params(theta, theta_size):\n",
    "    idx_W1_end = np.prod(theta_size[0])\n",
    "    idx_W2_end = idx_W1_end + np.prod(theta_size[1])\n",
    "    idx_b1_end = idx_W2_end + np.prod(theta_size[2])\n",
    "    W1 = theta[:idx_W1_end].reshape(theta_size[0])\n",
    "    W2 = theta[idx_W1_end:idx_W2_end].reshape(theta_size[1])\n",
    "    b1 = theta[idx_W2_end:idx_b1_end].reshape(theta_size[2])\n",
    "    b2 = theta[idx_b1_end:].reshape(theta_size[3])\n",
    "    return W1, W2, b1, b2\n",
    "\n",
    "def split_data(X, ratios, seed):\n",
    "    np.random.seed(seed)\n",
    "    m = X.shape[1]\n",
    "    indices = np.random.permutation(m)\n",
    "    train_end = int(ratios[0] * m)\n",
    "    val_end = train_end + int(ratios[1] * m)\n",
    "    train_idx = indices[:train_end]\n",
    "    val_idx = indices[train_end:val_end]\n",
    "    test_idx = indices[val_end:]\n",
    "    return X[:, train_idx], X[:, val_idx], X[:, test_idx]\n",
    "\n",
    "def predict_neural_network(theta, theta_size, X):\n",
    "    \n",
    "    W1, W2, b1, b2 = theta_to_params(theta, theta_size) \n",
    "\n",
    "    # =============== Your code here ============================\n",
    "    # Calculate the feedforward of the hidden layer and output layer. \n",
    "    # Remember to use the sigmoid function. The output preds is a vector of length m\n",
    "    # with the predicted class (1-10) for each input example where the predicted\n",
    "    # class is the argmax of the output layer\n",
    "    \n",
    "    z2 = np.dot(W1, X) + b1\n",
    "    a2 = sigmoid(z2)\n",
    "    z3 = np.dot(W2, a2) + b2\n",
    "    a3 = sigmoid(z3)\n",
    "    \n",
    "    \n",
    "    # =============================================================\n",
    "    return np.argmax(a3, axis=0) + 1\n",
    "\n",
    "# Load and split data\n",
    "data = scipy.io.loadmat(r'./../datasets/smallMNIST.mat')\n",
    "a=data['X']\n",
    "print(a.shape)\n",
    "X = data['X'].T\n",
    "y = data['y'].reshape(-1, 1).T\n",
    "print(np.unique(y)) # Note that the label vector y starts from 1\n",
    "Xtrain, Xval, Xtest = split_data(X, [0.6, 0.3, 0.1], 0)\n",
    "ytrain, yval, ytest = split_data(y, [0.6, 0.3, 0.1], 0)\n",
    "\n",
    "# Initialize the model\n",
    "num_vis = X.shape[0]\n",
    "num_out = len(np.unique(y))\n",
    "theta, theta_size = init_nn_parameters(num_vis, 50, num_out)\n",
    "\n",
    "# Train the model\n",
    "cost_function = lambda p: cost_neural_network(p, theta_size, Xtrain, ytrain)\n",
    "result = minimize(fun=cost_function, x0=theta, jac=True, method='L-BFGS-B', options={'maxiter': 100, 'disp': True})\n",
    "optTheta = result.x\n",
    "\n",
    "# Calculate the predictions of the trained model\n",
    "ypredtrain = predict_neural_network(optTheta, theta_size, Xtrain)\n",
    "ypredval = predict_neural_network(optTheta, theta_size, Xval)\n",
    "ypredtest = predict_neural_network(optTheta, theta_size, Xtest)\n",
    "\n",
    "# and the accuracies\n",
    "print('Train Set Accuracy:', np.mean(ypredtrain == ytrain) * 100)\n",
    "print('Val Set Accuracy:', np.mean(ypredval == yval) * 100)\n",
    "print('Test Set Accuracy:', np.mean(ypredtest == ytest) * 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Implement Auto-encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oNfRsN8oOwks",
    "outputId": "f3456344-7f37-4a26-a991-bac4dac18d08",
    "ExecuteTime": {
     "end_time": "2023-11-25T11:53:12.309685Z",
     "start_time": "2023-11-25T11:53:12.280446Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient difference: 3.1329041142685815e-10\n"
     ]
    }
   ],
   "source": [
    "# Time to implement a non-regularized auto-encoder.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_gradient(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "def theta_to_params(theta, theta_size):\n",
    "    idx_W1_end = np.prod(theta_size[0])\n",
    "    idx_W2_end = idx_W1_end + np.prod(theta_size[1])\n",
    "    idx_b1_end = idx_W2_end + np.prod(theta_size[2])\n",
    "    W1 = theta[:idx_W1_end].reshape(theta_size[0])\n",
    "    W2 = theta[idx_W1_end:idx_W2_end].reshape(theta_size[1])\n",
    "    b1 = theta[idx_W2_end:idx_b1_end].reshape(theta_size[2])\n",
    "    b2 = theta[idx_b1_end:].reshape(theta_size[3])\n",
    "    return W1, W2, b1, b2\n",
    "\n",
    "def cost_autoencoder(theta, theta_size, X):\n",
    "    m, n = X.shape\n",
    "\n",
    "    W1, W2, b1, b2 = theta_to_params(theta, theta_size)  \n",
    "    \n",
    "    # We expect the input to be equal to output, we thus use X as the output and input for the training.\n",
    "    y_mat = X\n",
    "    \n",
    "    # =============== Your code here ============================\n",
    "    # You can copy the code from cost_neural_network and work from there\n",
    "    z2 = np.dot(W1, X) + b1\n",
    "    a2 = sigmoid(z2)\n",
    "    z3 = np.dot(W2, a2) + b2\n",
    "    a3 = sigmoid(z3)\n",
    "    \n",
    "    J = np.sum(0.5 * np.sum((a3 - y_mat) ** 2))\n",
    "    \n",
    "    # Calculate delta error terms\n",
    "    delta3 = (a3 - y_mat) * a3 * (1 - a3)\n",
    "    delta2 = np.dot(W2.T, delta3) * a2 * (1 - a2)\n",
    "\n",
    "    # Calculate gradients\n",
    "    gradW2 = np.dot(delta3, a2.T)\n",
    "    gradW1 = np.dot(delta2, X.T)\n",
    "    gradb2 = np.sum(delta3, axis=1)\n",
    "    gradb1 = np.sum(delta2, axis=1)\n",
    "    \n",
    "    # ============================================================\n",
    "    \n",
    "    # Unroll gradients\n",
    "    grad = np.concatenate([gradW1.ravel(), gradW2.ravel(), gradb1.ravel(), gradb2.ravel()])\n",
    "    \n",
    "    return J, grad\n",
    "\n",
    "# Usage for autoencoder\n",
    "theta, theta_size = init_nn_parameters(8, 5, 8)  # Notice the third parameter is 8 for the autoencoder\n",
    "X = np.random.randn(8, 100)  # Input data\n",
    "\n",
    "# Initialize theta and theta_size appropriately before this\n",
    "cost, grad = cost_autoencoder(theta, theta_size, X)\n",
    "\n",
    "# Perform gradient checking as before\n",
    "numGrad = check_gradient(lambda p: cost_autoencoder(p, theta_size, X)[0], theta)\n",
    "\n",
    "# Compute the relative difference between the gradients\n",
    "diff = np.linalg.norm(numGrad - grad) / np.linalg.norm(numGrad + grad)\n",
    "\n",
    "print(\"Gradient difference:\", diff)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 - Reconsructing with Auto-encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "gd-44zQJwMcn",
    "outputId": "7b33935f-53bb-4a9b-d855-28828ca64626",
    "ExecuteTime": {
     "end_time": "2023-11-25T11:49:16.726032Z",
     "start_time": "2023-11-25T11:49:01.004926Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape (5000, 400)\n",
      "Xtrain shape, Xtemp shape  (3000, 400) (2000, 400)\n",
      "Xtrain.T shape (400, 3000)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[79], line 38\u001B[0m\n\u001B[1;32m     36\u001B[0m cost_function \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mlambda\u001B[39;00m p: cost_autoencoder(p, theta_size, Xtrain)\n\u001B[1;32m     37\u001B[0m \u001B[38;5;66;03m# use an optimizing function to get the optimal thetas\u001B[39;00m\n\u001B[0;32m---> 38\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[43mminimize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfun\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcost_function\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx0\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtheta\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mL-BFGS-B\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mjac\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmaxiter\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_iter\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdisp\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     39\u001B[0m opt_theta \u001B[38;5;241m=\u001B[39m result\u001B[38;5;241m.\u001B[39mx\n\u001B[1;32m     41\u001B[0m \u001B[38;5;66;03m# =============== Your code here ============================\u001B[39;00m\n\u001B[1;32m     42\u001B[0m \u001B[38;5;66;03m# Feedforward the training data on the trained auto-encoder to get the output layer Xrec.\u001B[39;00m\n\u001B[1;32m     43\u001B[0m \u001B[38;5;66;03m# You can use the function theta_to_params to get W1, W2, b1, b2 from opt_theta and theta_size\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/MLLab1/venv/lib/python3.9/site-packages/scipy/optimize/_minimize.py:710\u001B[0m, in \u001B[0;36mminimize\u001B[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001B[0m\n\u001B[1;32m    707\u001B[0m     res \u001B[38;5;241m=\u001B[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001B[1;32m    708\u001B[0m                              \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions)\n\u001B[1;32m    709\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m meth \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124ml-bfgs-b\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m--> 710\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43m_minimize_lbfgsb\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfun\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx0\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mjac\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbounds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    711\u001B[0m \u001B[43m                           \u001B[49m\u001B[43mcallback\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallback\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    712\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m meth \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtnc\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m    713\u001B[0m     res \u001B[38;5;241m=\u001B[39m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001B[38;5;241m=\u001B[39mcallback,\n\u001B[1;32m    714\u001B[0m                         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions)\n",
      "File \u001B[0;32m~/PycharmProjects/MLLab1/venv/lib/python3.9/site-packages/scipy/optimize/_lbfgsb_py.py:365\u001B[0m, in \u001B[0;36m_minimize_lbfgsb\u001B[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001B[0m\n\u001B[1;32m    359\u001B[0m task_str \u001B[38;5;241m=\u001B[39m task\u001B[38;5;241m.\u001B[39mtobytes()\n\u001B[1;32m    360\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m task_str\u001B[38;5;241m.\u001B[39mstartswith(\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mFG\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[1;32m    361\u001B[0m     \u001B[38;5;66;03m# The minimization routine wants f and g at the current x.\u001B[39;00m\n\u001B[1;32m    362\u001B[0m     \u001B[38;5;66;03m# Note that interruptions due to maxfun are postponed\u001B[39;00m\n\u001B[1;32m    363\u001B[0m     \u001B[38;5;66;03m# until the completion of the current minimization iteration.\u001B[39;00m\n\u001B[1;32m    364\u001B[0m     \u001B[38;5;66;03m# Overwrite f and g:\u001B[39;00m\n\u001B[0;32m--> 365\u001B[0m     f, g \u001B[38;5;241m=\u001B[39m \u001B[43mfunc_and_grad\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    366\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m task_str\u001B[38;5;241m.\u001B[39mstartswith(\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mNEW_X\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[1;32m    367\u001B[0m     \u001B[38;5;66;03m# new iteration\u001B[39;00m\n\u001B[1;32m    368\u001B[0m     n_iterations \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[0;32m~/PycharmProjects/MLLab1/venv/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py:285\u001B[0m, in \u001B[0;36mScalarFunction.fun_and_grad\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    283\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m np\u001B[38;5;241m.\u001B[39marray_equal(x, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mx):\n\u001B[1;32m    284\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_update_x_impl(x)\n\u001B[0;32m--> 285\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_update_fun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    286\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_update_grad()\n\u001B[1;32m    287\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mf, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mg\n",
      "File \u001B[0;32m~/PycharmProjects/MLLab1/venv/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py:251\u001B[0m, in \u001B[0;36mScalarFunction._update_fun\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    249\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_update_fun\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    250\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mf_updated:\n\u001B[0;32m--> 251\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_update_fun_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    252\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mf_updated \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/MLLab1/venv/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py:155\u001B[0m, in \u001B[0;36mScalarFunction.__init__.<locals>.update_fun\u001B[0;34m()\u001B[0m\n\u001B[1;32m    154\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mupdate_fun\u001B[39m():\n\u001B[0;32m--> 155\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mf \u001B[38;5;241m=\u001B[39m \u001B[43mfun_wrapped\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/MLLab1/venv/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py:137\u001B[0m, in \u001B[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001B[0;34m(x)\u001B[0m\n\u001B[1;32m    133\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnfev \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    134\u001B[0m \u001B[38;5;66;03m# Send a copy because the user may overwrite it.\u001B[39;00m\n\u001B[1;32m    135\u001B[0m \u001B[38;5;66;03m# Overwriting results in undefined behaviour because\u001B[39;00m\n\u001B[1;32m    136\u001B[0m \u001B[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001B[39;00m\n\u001B[0;32m--> 137\u001B[0m fx \u001B[38;5;241m=\u001B[39m \u001B[43mfun\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcopy\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    138\u001B[0m \u001B[38;5;66;03m# Make sure the function returns a true scalar\u001B[39;00m\n\u001B[1;32m    139\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m np\u001B[38;5;241m.\u001B[39misscalar(fx):\n",
      "File \u001B[0;32m~/PycharmProjects/MLLab1/venv/lib/python3.9/site-packages/scipy/optimize/_optimize.py:77\u001B[0m, in \u001B[0;36mMemoizeJac.__call__\u001B[0;34m(self, x, *args)\u001B[0m\n\u001B[1;32m     75\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, x, \u001B[38;5;241m*\u001B[39margs):\n\u001B[1;32m     76\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\" returns the function value \"\"\"\u001B[39;00m\n\u001B[0;32m---> 77\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_compute_if_needed\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     78\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_value\n",
      "File \u001B[0;32m~/PycharmProjects/MLLab1/venv/lib/python3.9/site-packages/scipy/optimize/_optimize.py:71\u001B[0m, in \u001B[0;36mMemoizeJac._compute_if_needed\u001B[0;34m(self, x, *args)\u001B[0m\n\u001B[1;32m     69\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m np\u001B[38;5;241m.\u001B[39mall(x \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mx) \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_value \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mjac \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     70\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mx \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39masarray(x)\u001B[38;5;241m.\u001B[39mcopy()\n\u001B[0;32m---> 71\u001B[0m     fg \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfun\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     72\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mjac \u001B[38;5;241m=\u001B[39m fg[\u001B[38;5;241m1\u001B[39m]\n\u001B[1;32m     73\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_value \u001B[38;5;241m=\u001B[39m fg[\u001B[38;5;241m0\u001B[39m]\n",
      "Cell \u001B[0;32mIn[79], line 36\u001B[0m, in \u001B[0;36m<lambda>\u001B[0;34m(p)\u001B[0m\n\u001B[1;32m     33\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mXtrain.T shape\u001B[39m\u001B[38;5;124m\"\u001B[39m, Xtrain\u001B[38;5;241m.\u001B[39mshape)\n\u001B[1;32m     35\u001B[0m \u001B[38;5;66;03m# Train the auto-encoder\u001B[39;00m\n\u001B[0;32m---> 36\u001B[0m cost_function \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mlambda\u001B[39;00m p: \u001B[43mcost_autoencoder\u001B[49m\u001B[43m(\u001B[49m\u001B[43mp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtheta_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mXtrain\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     37\u001B[0m \u001B[38;5;66;03m# use an optimizing function to get the optimal thetas\u001B[39;00m\n\u001B[1;32m     38\u001B[0m result \u001B[38;5;241m=\u001B[39m minimize(fun\u001B[38;5;241m=\u001B[39mcost_function, x0\u001B[38;5;241m=\u001B[39mtheta, method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mL-BFGS-B\u001B[39m\u001B[38;5;124m'\u001B[39m, jac\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, options\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmaxiter\u001B[39m\u001B[38;5;124m'\u001B[39m: num_iter, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdisp\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;28;01mFalse\u001B[39;00m})\n",
      "Cell \u001B[0;32mIn[78], line 43\u001B[0m, in \u001B[0;36mcost_autoencoder\u001B[0;34m(theta, theta_size, X)\u001B[0m\n\u001B[1;32m     40\u001B[0m delta2 \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mdot(W2\u001B[38;5;241m.\u001B[39mT, delta3) \u001B[38;5;241m*\u001B[39m a2 \u001B[38;5;241m*\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m a2)\n\u001B[1;32m     42\u001B[0m \u001B[38;5;66;03m# Calculate gradients\u001B[39;00m\n\u001B[0;32m---> 43\u001B[0m gradW2 \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdot\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdelta3\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43ma2\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mT\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     44\u001B[0m gradW1 \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mdot(delta2, X\u001B[38;5;241m.\u001B[39mT)\n\u001B[1;32m     45\u001B[0m gradb2 \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39msum(delta3, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "def theta_to_params(theta, theta_size):\n",
    "    idx_W1_end = np.prod(theta_size[0])\n",
    "    idx_W2_end = idx_W1_end + np.prod(theta_size[1])\n",
    "    idx_b1_end = idx_W2_end + np.prod(theta_size[2])\n",
    "    W1 = theta[:idx_W1_end].reshape(theta_size[0])\n",
    "    W2 = theta[idx_W1_end:idx_W2_end].reshape(theta_size[1])\n",
    "    b1 = theta[idx_W2_end:idx_b1_end].reshape(theta_size[2])\n",
    "    b2 = theta[idx_b1_end:].reshape(theta_size[3])\n",
    "    return W1, W2, b1, b2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = scipy.io.loadmat(r'./../datasets/smallMNIST.mat')\n",
    "X = data['X']\n",
    "print(\"x shape\",X.shape)\n",
    "\n",
    "Xtrain, Xtemp = train_test_split(X, test_size=0.4, random_state=0)\n",
    "Xval, Xtest = train_test_split(Xtemp, test_size=0.5, random_state=0)\n",
    "print(\"Xtrain shape, Xtemp shape \", Xtrain.shape,Xtemp.shape)\n",
    "\n",
    "# Decide a value for number of hidden units in the auto-encoder, num_hid,\n",
    "# and the number of training iterations, num_iter. \n",
    "# Try different values until you get good results\n",
    "num_hid = 35\n",
    "num_iter = 8000\n",
    "\n",
    "num_vis = Xtrain.shape[1]\n",
    "theta, theta_size = init_nn_parameters(num_vis, num_hid, num_vis)\n",
    "Xtrain = Xtrain.T\n",
    "print(\"Xtrain.T shape\", Xtrain.shape)\n",
    "\n",
    "# Train the auto-encoder\n",
    "cost_function = lambda p: cost_autoencoder(p, theta_size, Xtrain)\n",
    "# use an optimizing function to get the optimal thetas\n",
    "result = minimize(fun=cost_function, x0=theta, method='L-BFGS-B', jac=True, options={'maxiter': num_iter, 'disp': False})\n",
    "opt_theta = result.x\n",
    "\n",
    "# =============== Your code here ============================\n",
    "# Feedforward the training data on the trained auto-encoder to get the output layer Xrec.\n",
    "# You can use the function theta_to_params to get W1, W2, b1, b2 from opt_theta and theta_size\n",
    "\n",
    "W1, W2, b1, b2 = theta_to_params(opt_theta, theta_size)\n",
    "\n",
    "\n",
    "z2 = np.dot(W1, Xtrain) + b1\n",
    "a2 = sigmoid(z2)\n",
    "z3 = np.dot(W2, a2) + b2\n",
    "a3 = sigmoid(z3)\n",
    "\n",
    "Xrec = a3\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "# Plot 10x10 images from the original data and the reconstructed data\n",
    "def display_data(data, title):\n",
    "    fig, axarr = plt.subplots(10, 10, figsize=(10, 10))\n",
    "    for i in range(10):\n",
    "        for j in range(10):\n",
    "            axarr[i, j].imshow(data[:, i * 10 + j].reshape(20, 20).T, cmap='gray')\n",
    "            axarr[i, j].axis('off')\n",
    "    plt.suptitle(title)\n",
    "    plt.show()\n",
    "\n",
    "# Display original images\n",
    "print(\"Xtrain.T shape emd\", Xtrain.shape)\n",
    "display_data(Xtrain, 'Original input')\n",
    "\n",
    "# Display reconstructed images\n",
    "display_data(Xrec, 'Reconstructions')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises for Pass with distinction (VG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5 - Bias-variance analysis on the number of hidden units in a Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YjcR7eLuMNxK",
    "outputId": "b2929695-170e-4611-d722-c1fabbd41330",
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-11-25T12:03:22.293849Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hj/yrzp_6d94g91tf5mbs7ysj_80000gn/T/ipykernel_84020/2494404124.py:6: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n"
     ]
    }
   ],
   "source": [
    "# Perform a bias-variance analysis on the number of\n",
    "# hidden units in the neural network (numhid) to decide the optimal value. Show the\n",
    "# bias-variance analysis plot and describe what is the best choice.\n",
    "\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from scipy.optimize import minimize\n",
    "import scipy.io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# choose 10 different number for your hidden layer and decide the optimal value throught the plot\n",
    "numhidlist = range(1, 50, 1) # You can change these values to try different values of numhid\n",
    "\n",
    "ypredtrain = np.zeros(len(numhidlist))\n",
    "ypredval = np.zeros(len(numhidlist))\n",
    "ypredtest = np.zeros(len(numhidlist))\n",
    "\n",
    "# Load and split data\n",
    "data = scipy.io.loadmat(r'./../datasets/smallMNIST.mat')\n",
    "X = data['X'].T\n",
    "y = data['y'].reshape(-1, 1).T\n",
    "Xtrain, Xval, Xtest = split_data(X, [0.6, 0.3, 0.1], 0)\n",
    "ytrain, yval, ytest = split_data(y, [0.6, 0.3, 0.1], 0)\n",
    "\n",
    "num_vis = Xtrain.shape[0]\n",
    "num_out = len(np.unique(ytrain))\n",
    "\n",
    "\n",
    "for i, numhid in enumerate(numhidlist):\n",
    "    theta, theta_size = init_nn_parameters(num_vis, numhid, num_out)\n",
    "\n",
    "    cost_function = lambda p: cost_neural_network(p, theta_size, Xtrain, ytrain)\n",
    "\n",
    "    # =============== Your code here ============================\n",
    "    # Chose a minimizer function from scipy in order to get the optimal theta\n",
    "    \n",
    "    num_iter = 500\n",
    "    result = minimize(fun=cost_function, x0=theta, method='L-BFGS-B', jac=True, options={'maxiter': num_iter, 'disp': False})\n",
    "    opt_theta = result.x\n",
    "    \n",
    "    # ========================================================\n",
    "\n",
    "    ypredtrain[i] = np.mean(predict_neural_network(opt_theta, theta_size, Xtrain) == ytrain) * 100\n",
    "    ypredval[i] = np.mean(predict_neural_network(opt_theta, theta_size, Xval) == yval) * 100\n",
    "    ypredtest[i] = np.mean(predict_neural_network(opt_theta, theta_size, Xtest) == ytest) * 100\n",
    "\n",
    "\n",
    "# =============== Your code here ============================\n",
    "# Plot the results of ypredtrain and ypredval\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(numhidlist, ypredtrain, label='Training Accuracy')\n",
    "plt.plot(numhidlist, ypredval, label='Validation Accuracy')\n",
    "\n",
    "plt.title('Model Accuracy vs Number of Hidden Nodes')\n",
    "plt.xlabel('Number of Hidden Nodes')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend(loc='best')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# ========================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6 - Implement Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T11:49:16.728236Z",
     "start_time": "2023-11-25T11:49:16.728113Z"
    }
   },
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    e_z = np.exp(z - np.max(z, axis=0, keepdims=True))\n",
    "    h = e_z / e_z.sum(axis=0, keepdims=True)\n",
    "    return h\n",
    "\n",
    "def check_gradient(J, theta):\n",
    "    numgrad = np.zeros_like(theta)\n",
    "    epsilon = 1e-4\n",
    "\n",
    "    for i in range(len(numgrad)):\n",
    "        e = np.zeros_like(theta).flatten()\n",
    "        e[i] = epsilon\n",
    "        numgrad[i] = (J(theta + e.reshape(theta.shape)) - J(theta - e.reshape(theta.shape))) / (2 * epsilon)\n",
    "\n",
    "    return numgrad.flatten()\n",
    "\n",
    "def init_softmax_parameters(numin, numout):\n",
    "    theta = np.random.randn(numout, numin)\n",
    "    theta = theta.ravel()\n",
    "    return theta\n",
    "\n",
    "def cost_softmax(theta, X, y, numClasses, lambda_=0):\n",
    "    m, n = X.shape\n",
    "    lambda_ = 0\n",
    "    theta = np.reshape(theta, (numClasses, X.shape[0]))    \n",
    "    y_mat = np.eye(num_classes)[y.reshape(-1)-1].T\n",
    "    \n",
    "    # =============== Your code here ============================\n",
    "    # Calculate the hypothesis h using theta, X, and the function softmax. Use h to calculate J and grad. \n",
    "    # Remember to .ravel() the grad just as we did for the cost_neural_network\n",
    "    \n",
    "    # =============================================================\n",
    "    \n",
    "    return J, grad\n",
    "\n",
    "def predict_softmax(theta, X, numClasses):\n",
    "    \n",
    "    # =============== Your code here ============================\n",
    "    # Use similar as predict_neural_network\n",
    "    \n",
    "    # =============================================================\n",
    "    return preds\n",
    "\n",
    "\n",
    "# Create test data to check gradients\n",
    "num_classes = 10\n",
    "num_inputs = 5\n",
    "num_samples = 20\n",
    "X = np.random.randn(num_inputs, num_samples)\n",
    "y = np.random.randint(1, num_classes+1, size=(1, num_samples))\n",
    "print(X.shape,y.shape)\n",
    "\n",
    "theta = init_softmax_parameters(num_inputs, num_classes)\n",
    "cost, grad = cost_softmax(theta, X, y[0], num_classes)  # Adjust indexing here since y[0] gives the 1D array\n",
    "numGrad = check_gradient(lambda p: cost_softmax(p, X, y[0], num_classes)[0], theta)  # Adjust indexing here too\n",
    "diff = np.linalg.norm(numGrad - grad) / np.linalg.norm(numGrad + grad)\n",
    "print(diff)\n",
    "\n",
    "\n",
    "# =============== Your code here ============================\n",
    "# Use the softmax on the raw MNISTsmall dataset\n",
    "\n",
    "# =============================================================\n",
    "\n",
    "\n",
    "# =============== Your code here ============================\n",
    "# Use the softmax on the trained auto-encoder hidden layer\n",
    "\n",
    "# =============================================================\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7 - PyTorch MNIST Classification Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 401
    },
    "id": "ujMUeHrPw2GW",
    "outputId": "7f42334a-c37c-47d1-ab7f-bc9e93c60bf8",
    "ExecuteTime": {
     "end_time": "2023-11-25T11:49:16.728484Z",
     "start_time": "2023-11-25T11:49:16.728288Z"
    }
   },
   "outputs": [],
   "source": [
    "# PyTorch MNIST Classification Tutorial\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# We apply a transform to normalize the data.\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Download and load the training data\n",
    "trainset = torchvision.datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Download and load the test data\n",
    "testset = torchvision.datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Create your own neural network now\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # =============== Your code here ============================\n",
    "        # Define your layers here with nn.Linear (optional: Try nn.Conv2d for a CNN)\n",
    "        \n",
    "        # ========================================================\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28) # Flatten the image\n",
    "        \n",
    "        # =============== Your code here ==========================\n",
    "        # Choose the activations functions here for example torch.sigmoid or torch.relu \n",
    "        # and use the previously defined layers above\n",
    "\n",
    "        # ========================================================\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "\n",
    "#  Choose your loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "epochs = 15 #choose a number of epochs\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total}%')\n",
    "# Optional: Visualize some predictions\n",
    "# This will display images along with predicted and true labels.\n",
    "import numpy as np\n",
    "dataiter = iter(testloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# Function to show an image\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5  # Unnormalize the image\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))  # Convert from Tensor image\n",
    "\n",
    "# Show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print('GroundTruth: ', ' '.join(f'{labels[j].item()}' for j in range(10)))\n",
    "\n",
    "# Get predictions\n",
    "outputs = net(images.view(-1, 28*28))  # Reshape images to match the input shape of the network\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "print('  Predicted: ', ' '.join(f'{predicted[j].item()}'\n",
    "                              for j in range(10)))\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
