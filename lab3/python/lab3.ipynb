{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Machine Learning </center>\n",
    "## <center> Lab 3 </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises for Pass (G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Neural Network implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uYF6zY4QTqY5",
    "outputId": "38840d62-8fe5-4ea9-d0e7-03e3ffb0f15b",
    "ExecuteTime": {
     "end_time": "2023-11-24T13:09:44.155308Z",
     "start_time": "2023-11-24T13:09:44.114805Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 100) (1, 100)\n",
      "[(5, 8), (10, 5), (5, 1), (10, 1)]\n",
      "3.6167921578287413e-10\n"
     ]
    }
   ],
   "source": [
    "# Time to implement a 1-layered non-regularized neural network.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_gradient(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "def check_gradient(J, theta):\n",
    "    numgrad = np.zeros_like(theta)\n",
    "    epsilon = 1e-4\n",
    "\n",
    "    for i in range(len(numgrad)):\n",
    "        e = np.zeros_like(theta).flatten()\n",
    "        e[i] = epsilon\n",
    "        numgrad[i] = (J(theta + e.reshape(theta.shape)) - J(theta - e.reshape(theta.shape))) / (2 * epsilon)\n",
    "\n",
    "    return numgrad.flatten()\n",
    "\n",
    "def init_nn_parameters(numin, numhid, numout):\n",
    "    r1  = np.sqrt(6) / np.sqrt(numin+numhid+1)\n",
    "    r2  = np.sqrt(6) / np.sqrt(numin+numout+1)\n",
    "    W1 = np.random.rand(numhid, numin) * 2 * r1 - r1\n",
    "    W2 = np.random.rand(numout, numhid) * 2 * r2 - r2\n",
    "    b1 = np.zeros((numhid, 1))\n",
    "    b2 = np.zeros((numout, 1))\n",
    "    theta = np.concatenate([W1.ravel(), W2.ravel(), b1.ravel(), b2.ravel()])\n",
    "    theta_size = [W1.shape, W2.shape, b1.shape, b2.shape]\n",
    "    return theta, theta_size\n",
    "\n",
    "def theta_to_params(theta, theta_size):\n",
    "    idx_W1_end = np.prod(theta_size[0])\n",
    "    idx_W2_end = idx_W1_end + np.prod(theta_size[1])\n",
    "    idx_b1_end = idx_W2_end + np.prod(theta_size[2])\n",
    "    W1 = theta[:idx_W1_end].reshape(theta_size[0])\n",
    "    W2 = theta[idx_W1_end:idx_W2_end].reshape(theta_size[1])\n",
    "    b1 = theta[idx_W2_end:idx_b1_end].reshape(theta_size[2])\n",
    "    b2 = theta[idx_b1_end:].reshape(theta_size[3])\n",
    "    return W1, W2, b1, b2\n",
    "\n",
    "def cost_neural_network(theta, theta_size, X, y):\n",
    "    m, n = X.shape\n",
    "    num_classes = len(np.unique(y))\n",
    "    lambda_ = 0\n",
    "\n",
    "    W1, W2, b1, b2 = theta_to_params(theta, theta_size) \n",
    "    y_mat = np.eye(num_classes)[y.reshape(-1)-1].T\n",
    "    \n",
    "    # =============== Your code here ============================\n",
    "    # Forward propagation\n",
    "    z2 = np.dot(W1, X) + b1\n",
    "    a2 = sigmoid(z2)\n",
    "    z3 = np.dot(W2, a2) + b2\n",
    "    a3 = sigmoid(z3)\n",
    "\n",
    "    # Calculate cost J\n",
    "    J = np.sum(0.5 * np.sum((a3 - y_mat) ** 2))\n",
    "\n",
    "    # Calculate delta error terms\n",
    "    delta3 = (a3 - y_mat) * a3 * (1 - a3)\n",
    "    delta2 = np.dot(W2.T, delta3) * a2 * (1 - a2)\n",
    "\n",
    "    # Calculate gradients\n",
    "    gradW2 = np.dot(delta3, a2.T)\n",
    "    gradW1 = np.dot(delta2, X.T)\n",
    "    gradb2 = np.sum(delta3, axis=1)\n",
    "    gradb1 = np.sum(delta2, axis=1)\n",
    "\n",
    "     # =============================================================\n",
    "    \n",
    "    # Unroll gradients\n",
    "    grad = np.concatenate([gradW1.ravel(), gradW2.ravel(), gradb1.ravel(), gradb2.ravel()])\n",
    "    \n",
    "    return J, grad\n",
    "\n",
    "# Create test data to check gradients\n",
    "X = np.random.randn(8, 100)\n",
    "y = np.random.randint(1, 11, size=(1, 100))\n",
    "print(X.shape,y.shape)\n",
    "\n",
    "theta, theta_size = init_nn_parameters(8, 5, 10)\n",
    "print(theta_size)\n",
    "cost, grad = cost_neural_network(theta, theta_size, X, y[0])  # Adjust indexing here since y[0] gives the 1D array\n",
    "numGrad = check_gradient(lambda p: cost_neural_network(p, theta_size, X, y[0])[0], theta)  # Adjust indexing here too\n",
    "diff = np.linalg.norm(numGrad - grad) / np.linalg.norm(numGrad + grad)\n",
    "print(diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Neural network for handwritten digit classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ofFQfRvVdMV4",
    "outputId": "71b0bf70-621f-4809-c009-6a7371357f64",
    "ExecuteTime": {
     "end_time": "2023-11-24T13:35:23.131051Z",
     "start_time": "2023-11-24T13:35:20.665Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 400)\n",
      "[ 1  2  3  4  5  6  7  8  9 10]\n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =        20560     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  3.57057D+03    |proj g|=  3.38764D+02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate    1    f=  1.60593D+03    |proj g|=  8.11629D+01\n",
      "\n",
      "At iterate    2    f=  1.40476D+03    |proj g|=  2.69524D+01\n",
      "\n",
      "At iterate    3    f=  1.35700D+03    |proj g|=  9.42081D+00\n",
      "\n",
      "At iterate    4    f=  1.34608D+03    |proj g|=  4.45356D+00\n",
      "\n",
      "At iterate    5    f=  1.34226D+03    |proj g|=  3.92513D+00\n",
      "\n",
      "At iterate    6    f=  1.33704D+03    |proj g|=  4.75438D+00\n",
      "\n",
      "At iterate    7    f=  1.32050D+03    |proj g|=  9.87330D+00\n",
      "\n",
      "At iterate    8    f=  1.20963D+03    |proj g|=  2.00337D+01\n",
      "\n",
      "At iterate    9    f=  1.18295D+03    |proj g|=  5.56320D+01\n",
      "\n",
      "At iterate   10    f=  9.36538D+02    |proj g|=  5.59308D+01\n",
      "\n",
      "At iterate   11    f=  8.12597D+02    |proj g|=  2.13916D+01\n",
      "\n",
      "At iterate   12    f=  7.83722D+02    |proj g|=  1.86567D+01\n",
      "\n",
      "At iterate   13    f=  7.59697D+02    |proj g|=  2.08649D+01\n",
      "\n",
      "At iterate   14    f=  7.17880D+02    |proj g|=  1.75096D+01\n",
      "\n",
      "At iterate   15    f=  6.68016D+02    |proj g|=  3.05300D+01\n",
      "\n",
      "At iterate   16    f=  6.23556D+02    |proj g|=  1.22986D+01\n",
      "\n",
      "At iterate   17    f=  6.00337D+02    |proj g|=  1.03523D+01\n",
      "\n",
      "At iterate   18    f=  5.81641D+02    |proj g|=  6.88754D+00\n",
      "\n",
      "At iterate   19    f=  5.52081D+02    |proj g|=  8.11250D+00\n",
      "\n",
      "At iterate   20    f=  5.24672D+02    |proj g|=  1.35420D+01\n",
      "\n",
      "At iterate   21    f=  5.09111D+02    |proj g|=  4.91029D+00\n",
      "\n",
      "At iterate   22    f=  4.93554D+02    |proj g|=  2.61168D+00\n",
      "\n",
      "At iterate   23    f=  4.82474D+02    |proj g|=  3.83523D+00\n",
      "\n",
      "At iterate   24    f=  4.63262D+02    |proj g|=  4.09563D+00\n",
      "\n",
      "At iterate   25    f=  4.52224D+02    |proj g|=  4.31896D+00\n",
      "\n",
      "At iterate   26    f=  4.40539D+02    |proj g|=  1.73834D+00\n",
      "\n",
      "At iterate   27    f=  4.32053D+02    |proj g|=  1.40314D+00\n",
      "\n",
      "At iterate   28    f=  4.19496D+02    |proj g|=  2.22465D+00\n",
      "\n",
      "At iterate   29    f=  4.07629D+02    |proj g|=  2.83112D+00\n",
      "\n",
      "At iterate   30    f=  3.95861D+02    |proj g|=  2.10014D+00\n",
      "\n",
      "At iterate   31    f=  3.87137D+02    |proj g|=  3.25013D+00\n",
      "\n",
      "At iterate   32    f=  3.78821D+02    |proj g|=  8.63853D+00\n",
      "\n",
      "At iterate   33    f=  3.73144D+02    |proj g|=  1.38104D+01\n",
      "\n",
      "At iterate   34    f=  3.61488D+02    |proj g|=  2.10335D+01\n",
      "\n",
      "At iterate   35    f=  3.45164D+02    |proj g|=  8.45595D+00\n",
      "\n",
      "At iterate   36    f=  3.31365D+02    |proj g|=  7.48800D+00\n",
      "\n",
      "At iterate   37    f=  3.09742D+02    |proj g|=  9.97973D+00\n",
      "\n",
      "At iterate   38    f=  2.94812D+02    |proj g|=  4.45501D+00\n",
      "\n",
      "At iterate   39    f=  2.84411D+02    |proj g|=  3.94099D+00\n",
      "\n",
      "At iterate   40    f=  2.71540D+02    |proj g|=  2.81260D+00\n",
      "\n",
      "At iterate   41    f=  2.62615D+02    |proj g|=  3.11830D+00\n",
      "\n",
      "At iterate   42    f=  2.56584D+02    |proj g|=  1.90458D+00\n",
      "\n",
      "At iterate   43    f=  2.50668D+02    |proj g|=  1.65456D+00\n",
      "\n",
      "At iterate   44    f=  2.45286D+02    |proj g|=  1.42062D+00\n",
      "\n",
      "At iterate   45    f=  2.39588D+02    |proj g|=  1.54794D+00\n",
      "\n",
      "At iterate   46    f=  2.34389D+02    |proj g|=  1.26374D+00\n",
      "\n",
      "At iterate   47    f=  2.30284D+02    |proj g|=  1.29158D+00\n",
      "\n",
      "At iterate   48    f=  2.24994D+02    |proj g|=  1.31143D+00\n",
      "\n",
      "At iterate   49    f=  2.22987D+02    |proj g|=  2.87970D+00\n",
      "\n",
      "At iterate   50    f=  2.19898D+02    |proj g|=  8.87268D-01\n",
      "\n",
      "At iterate   51    f=  2.17611D+02    |proj g|=  9.98606D-01\n",
      "\n",
      "At iterate   52    f=  2.14671D+02    |proj g|=  1.17812D+00\n",
      "\n",
      "At iterate   53    f=  2.11897D+02    |proj g|=  9.48199D-01\n",
      "\n",
      "At iterate   54    f=  2.09030D+02    |proj g|=  5.28972D-01\n",
      "\n",
      "At iterate   55    f=  2.07601D+02    |proj g|=  3.80743D-01\n",
      "\n",
      "At iterate   56    f=  2.05708D+02    |proj g|=  5.43436D-01\n",
      "\n",
      "At iterate   57    f=  2.04197D+02    |proj g|=  5.06207D-01\n",
      "\n",
      "At iterate   58    f=  2.02457D+02    |proj g|=  5.46517D-01\n",
      "\n",
      "At iterate   59    f=  2.01045D+02    |proj g|=  2.19091D-01\n",
      "\n",
      "At iterate   60    f=  2.00085D+02    |proj g|=  2.13076D-01\n",
      "\n",
      "At iterate   61    f=  1.99125D+02    |proj g|=  4.79737D-01\n",
      "\n",
      "At iterate   62    f=  1.98159D+02    |proj g|=  4.45945D-01\n",
      "\n",
      "At iterate   63    f=  1.97328D+02    |proj g|=  2.21376D-01\n",
      "\n",
      "At iterate   64    f=  1.96588D+02    |proj g|=  1.94986D-01\n",
      "\n",
      "At iterate   65    f=  1.95951D+02    |proj g|=  2.36724D-01\n",
      "\n",
      "At iterate   66    f=  1.95510D+02    |proj g|=  1.69900D-01\n",
      "\n",
      "At iterate   67    f=  1.95207D+02    |proj g|=  1.15627D-01\n",
      "\n",
      "At iterate   68    f=  1.94517D+02    |proj g|=  1.57817D-01\n",
      "\n",
      "At iterate   69    f=  1.94267D+02    |proj g|=  1.99687D-01\n",
      "\n",
      "At iterate   70    f=  1.93878D+02    |proj g|=  1.32621D-01\n",
      "\n",
      "At iterate   71    f=  1.93087D+02    |proj g|=  2.68176D-01\n",
      "\n",
      "At iterate   72    f=  1.92552D+02    |proj g|=  2.40668D-01\n",
      "\n",
      "At iterate   73    f=  1.92022D+02    |proj g|=  2.02847D-01\n",
      "\n",
      "At iterate   74    f=  1.91505D+02    |proj g|=  1.40319D-01\n",
      "\n",
      "At iterate   75    f=  1.91292D+02    |proj g|=  1.99158D-01\n",
      "\n",
      "At iterate   76    f=  1.91126D+02    |proj g|=  1.06281D-01\n",
      "\n",
      "At iterate   77    f=  1.90926D+02    |proj g|=  1.39570D-01\n",
      "\n",
      "At iterate   78    f=  1.90546D+02    |proj g|=  1.38210D-01\n",
      "\n",
      "At iterate   79    f=  1.90434D+02    |proj g|=  1.36812D-01\n",
      "\n",
      "At iterate   80    f=  1.90211D+02    |proj g|=  2.32510D-01\n",
      "\n",
      "At iterate   81    f=  1.90126D+02    |proj g|=  1.74900D-01\n",
      "\n",
      "At iterate   82    f=  1.89953D+02    |proj g|=  1.19875D-01\n",
      "\n",
      "At iterate   83    f=  1.89786D+02    |proj g|=  6.50234D-02\n",
      "\n",
      "At iterate   84    f=  1.89659D+02    |proj g|=  4.39573D-02\n",
      "\n",
      "At iterate   85    f=  1.88053D+02    |proj g|=  1.97503D-01\n",
      "\n",
      "At iterate   86    f=  1.87858D+02    |proj g|=  3.51630D-01\n",
      "\n",
      "At iterate   87    f=  1.87513D+02    |proj g|=  4.00031D-01\n",
      "\n",
      "At iterate   88    f=  1.86557D+02    |proj g|=  2.51442D-01\n",
      "\n",
      "At iterate   89    f=  1.86259D+02    |proj g|=  1.76556D-01\n",
      "\n",
      "At iterate   90    f=  1.86083D+02    |proj g|=  2.93615D-01\n",
      "\n",
      "At iterate   91    f=  1.85881D+02    |proj g|=  8.09001D-02\n",
      "\n",
      "At iterate   92    f=  1.85843D+02    |proj g|=  7.38242D-02\n",
      "\n",
      "At iterate   93    f=  1.85705D+02    |proj g|=  4.28385D-02\n",
      "\n",
      "At iterate   94    f=  1.85620D+02    |proj g|=  2.61562D-02\n",
      "\n",
      "At iterate   95    f=  1.85129D+02    |proj g|=  4.61502D-02\n",
      "\n",
      "At iterate   96    f=  1.85105D+02    |proj g|=  3.43949D-02\n",
      "\n",
      "At iterate   97    f=  1.85065D+02    |proj g|=  2.04573D-02\n",
      "\n",
      "At iterate   98    f=  1.85052D+02    |proj g|=  6.21222D-03\n",
      "\n",
      "At iterate   99    f=  1.85043D+02    |proj g|=  8.26961D-03\n",
      "\n",
      "At iterate  100    f=  1.84656D+02    |proj g|=  5.23617D-02\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "20560    100    122      1     0     0   5.236D-02   1.847D+02\n",
      "  F =   184.65639386231879     \n",
      "\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 \n",
      "Train Set Accuracy: 88.13333333333333\n",
      "Val Set Accuracy: 83.13333333333334\n",
      "Test Set Accuracy: 85.8\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from scipy.optimize import minimize\n",
    "import scipy.io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def theta_to_params(theta, theta_size):\n",
    "    idx_W1_end = np.prod(theta_size[0])\n",
    "    idx_W2_end = idx_W1_end + np.prod(theta_size[1])\n",
    "    idx_b1_end = idx_W2_end + np.prod(theta_size[2])\n",
    "    W1 = theta[:idx_W1_end].reshape(theta_size[0])\n",
    "    W2 = theta[idx_W1_end:idx_W2_end].reshape(theta_size[1])\n",
    "    b1 = theta[idx_W2_end:idx_b1_end].reshape(theta_size[2])\n",
    "    b2 = theta[idx_b1_end:].reshape(theta_size[3])\n",
    "    return W1, W2, b1, b2\n",
    "\n",
    "def split_data(X, ratios, seed):\n",
    "    np.random.seed(seed)\n",
    "    m = X.shape[1]\n",
    "    indices = np.random.permutation(m)\n",
    "    train_end = int(ratios[0] * m)\n",
    "    val_end = train_end + int(ratios[1] * m)\n",
    "    train_idx = indices[:train_end]\n",
    "    val_idx = indices[train_end:val_end]\n",
    "    test_idx = indices[val_end:]\n",
    "    return X[:, train_idx], X[:, val_idx], X[:, test_idx]\n",
    "\n",
    "def predict_neural_network(theta, theta_size, X):\n",
    "    \n",
    "    W1, W2, b1, b2 = theta_to_params(theta, theta_size) \n",
    "\n",
    "    # =============== Your code here ============================\n",
    "    # Calculate the feedforward of the hidden layer and output layer. \n",
    "    # Remember to use the sigmoid function. The output preds is a vector of length m\n",
    "    # with the predicted class (1-10) for each input example where the predicted\n",
    "    # class is the argmax of the output layer\n",
    "    \n",
    "    z2 = np.dot(W1, X) + b1\n",
    "    a2 = sigmoid(z2)\n",
    "    z3 = np.dot(W2, a2) + b2\n",
    "    a3 = sigmoid(z3)\n",
    "    \n",
    "    \n",
    "    # =============================================================\n",
    "    return np.argmax(a3, axis=0) + 1\n",
    "\n",
    "# Load and split data\n",
    "data = scipy.io.loadmat(r'./../datasets/smallMNIST.mat')\n",
    "a=data['X']\n",
    "print(a.shape)\n",
    "X = data['X'].T\n",
    "y = data['y'].reshape(-1, 1).T\n",
    "print(np.unique(y)) # Note that the label vector y starts from 1\n",
    "Xtrain, Xval, Xtest = split_data(X, [0.6, 0.3, 0.1], 0)\n",
    "ytrain, yval, ytest = split_data(y, [0.6, 0.3, 0.1], 0)\n",
    "\n",
    "# Initialize the model\n",
    "num_vis = X.shape[0]\n",
    "num_out = len(np.unique(y))\n",
    "theta, theta_size = init_nn_parameters(num_vis, 50, num_out)\n",
    "\n",
    "# Train the model\n",
    "cost_function = lambda p: cost_neural_network(p, theta_size, Xtrain, ytrain)\n",
    "result = minimize(fun=cost_function, x0=theta, jac=True, method='L-BFGS-B', options={'maxiter': 100, 'disp': True})\n",
    "optTheta = result.x\n",
    "\n",
    "# Calculate the predictions of the trained model\n",
    "ypredtrain = predict_neural_network(optTheta, theta_size, Xtrain)\n",
    "ypredval = predict_neural_network(optTheta, theta_size, Xval)\n",
    "ypredtest = predict_neural_network(optTheta, theta_size, Xtest)\n",
    "\n",
    "# and the accuracies\n",
    "print('Train Set Accuracy:', np.mean(ypredtrain == ytrain) * 100)\n",
    "print('Val Set Accuracy:', np.mean(ypredval == yval) * 100)\n",
    "print('Test Set Accuracy:', np.mean(ypredtest == ytest) * 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Implement Auto-encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oNfRsN8oOwks",
    "outputId": "f3456344-7f37-4a26-a991-bac4dac18d08",
    "ExecuteTime": {
     "start_time": "2023-11-24T13:09:44.685302Z"
    }
   },
   "outputs": [],
   "source": [
    "# Time to implement a non-regularized auto-encoder.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_gradient(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "def theta_to_params(theta, theta_size):\n",
    "    idx_W1_end = np.prod(theta_size[0])\n",
    "    idx_W2_end = idx_W1_end + np.prod(theta_size[1])\n",
    "    idx_b1_end = idx_W2_end + np.prod(theta_size[2])\n",
    "    W1 = theta[:idx_W1_end].reshape(theta_size[0])\n",
    "    W2 = theta[idx_W1_end:idx_W2_end].reshape(theta_size[1])\n",
    "    b1 = theta[idx_W2_end:idx_b1_end].reshape(theta_size[2])\n",
    "    b2 = theta[idx_b1_end:].reshape(theta_size[3])\n",
    "    return W1, W2, b1, b2\n",
    "\n",
    "def cost_autoencoder(theta, theta_size, X):\n",
    "    m, n = X.shape\n",
    "\n",
    "    W1, W2, b1, b2 = theta_to_params(theta, theta_size)  \n",
    "    \n",
    "    # =============== Your code here ============================\n",
    "    # You can copy the code from cost_neural_network and work from there\n",
    "    \n",
    "    # ============================================================\n",
    "    \n",
    "    # Unroll gradients\n",
    "    grad = np.concatenate([gradW1.ravel(), gradW2.ravel(), gradb1.ravel(), gradb2.ravel()])\n",
    "    \n",
    "    return J, grad\n",
    "\n",
    "# Usage for autoencoder\n",
    "theta, theta_size = init_nn_parameters(8, 5, 8)  # Notice the third parameter is 8 for the autoencoder\n",
    "X = np.random.randn(8, 100)  # Input data\n",
    "\n",
    "# Initialize theta and theta_size appropriately before this\n",
    "cost, grad = cost_autoencoder(theta, theta_size, X)\n",
    "\n",
    "# Perform gradient checking as before\n",
    "numGrad = check_gradient(lambda p: cost_autoencoder(p, theta_size, X)[0], theta)\n",
    "\n",
    "# Compute the relative difference between the gradients\n",
    "diff = np.linalg.norm(numGrad - grad) / np.linalg.norm(numGrad + grad)\n",
    "\n",
    "print(\"Gradient difference:\", diff)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 - Reconsructing with Auto-encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "gd-44zQJwMcn",
    "outputId": "7b33935f-53bb-4a9b-d855-28828ca64626",
    "ExecuteTime": {
     "start_time": "2023-11-24T13:09:44.686930Z"
    }
   },
   "outputs": [],
   "source": [
    "def theta_to_params(theta, theta_size):\n",
    "    idx_W1_end = np.prod(theta_size[0])\n",
    "    idx_W2_end = idx_W1_end + np.prod(theta_size[1])\n",
    "    idx_b1_end = idx_W2_end + np.prod(theta_size[2])\n",
    "    W1 = theta[:idx_W1_end].reshape(theta_size[0])\n",
    "    W2 = theta[idx_W1_end:idx_W2_end].reshape(theta_size[1])\n",
    "    b1 = theta[idx_W2_end:idx_b1_end].reshape(theta_size[2])\n",
    "    b2 = theta[idx_b1_end:].reshape(theta_size[3])\n",
    "    return W1, W2, b1, b2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = scipy.io.loadmat(r'./../datasets/smallMNIST.mat')\n",
    "X = data['X']\n",
    "print(\"x shape\",X.shape)\n",
    "\n",
    "Xtrain, Xtemp = train_test_split(X, test_size=0.4, random_state=0)\n",
    "Xval, Xtest = train_test_split(Xtemp, test_size=0.5, random_state=0)\n",
    "print(\"Xtrain shape, Xtemp shape \", Xtrain.shape,Xtemp.shape)\n",
    "\n",
    "# Decide a value for number of hidden units in the auto-encoder, num_hid,\n",
    "# and the number of training iterations, num_iter. \n",
    "# Try different values until you get good results\n",
    "num_hid = 10\n",
    "num_iter = 10\n",
    "\n",
    "num_vis = Xtrain.shape[1]\n",
    "theta, theta_size = init_nn_parameters(num_vis, num_hid, num_vis)\n",
    "Xtrain = Xtrain.T\n",
    "print(\"Xtrain.T shape\", Xtrain.shape)\n",
    "\n",
    "# Train the auto-encoder\n",
    "cost_function = lambda p: cost_autoencoder(p, theta_size, Xtrain)\n",
    "# use an optimizing function to get the optimal thetas\n",
    "result = minimize(fun=cost_function, x0=theta, method='L-BFGS-B', jac=True, options={'maxiter': num_iter, 'disp': True})\n",
    "opt_theta = result.x\n",
    "\n",
    "# =============== Your code here ============================\n",
    "# Feedforward the training data on the trained auto-encoder to get the output layer Xrec.\n",
    "# You can use the function theta_to_params to get W1, W2, b1, b2 from opt_theta and theta_size\n",
    "\n",
    "Xrec = ...\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "# Plot 10x10 images from the original data and the reconstructed data\n",
    "def display_data(data, title):\n",
    "    fig, axarr = plt.subplots(10, 10, figsize=(10, 10))\n",
    "    for i in range(10):\n",
    "        for j in range(10):\n",
    "            axarr[i, j].imshow(data[:, i * 10 + j].reshape(20, 20).T, cmap='gray')\n",
    "            axarr[i, j].axis('off')\n",
    "    plt.suptitle(title)\n",
    "    plt.show()\n",
    "\n",
    "# Display original images\n",
    "print(\"Xtrain.T shape emd\", Xtrain.shape)\n",
    "display_data(Xtrain, 'Original input')\n",
    "\n",
    "# Display reconstructed images\n",
    "display_data(Xrec, 'Reconstructions')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises for Pass with distinction (VG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5 - Bias-variance analysis on the number of hidden units in a Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YjcR7eLuMNxK",
    "outputId": "b2929695-170e-4611-d722-c1fabbd41330",
    "ExecuteTime": {
     "start_time": "2023-11-24T13:09:44.688489Z"
    }
   },
   "outputs": [],
   "source": [
    "# Perform a bias-variance analysis on the number of\n",
    "# hidden units in the neural network (numhid) to decide the optimal value. Show the\n",
    "# bias-variance analysis plot and describe what is the best choice.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# choose 10 different number for your hidden layer and decide the optimal value throught the plot\n",
    "numhidlist = [1,2,3,4,5,6,7,8,9,10] # You can change these values to try different values of numhid\n",
    "\n",
    "ypredtrain = np.zeros(len(numhidlist))\n",
    "ypredval = np.zeros(len(numhidlist))\n",
    "ypredtest = np.zeros(len(numhidlist))\n",
    "\n",
    "# Load and split data\n",
    "data = scipy.io.loadmat(r'./../datasets/smallMNIST.mat')\n",
    "X = data['X'].T\n",
    "y = data['y'].reshape(-1, 1).T\n",
    "Xtrain, Xval, Xtest = split_data(X, [0.6, 0.3, 0.1], 0)\n",
    "ytrain, yval, ytest = split_data(y, [0.6, 0.3, 0.1], 0)\n",
    "\n",
    "num_vis = Xtrain.shape[0]\n",
    "num_out = len(np.unique(ytrain))\n",
    "\n",
    "\n",
    "for i, numhid in enumerate(numhidlist):\n",
    "    theta, theta_size = init_nn_parameters(num_vis, numhid, num_out)\n",
    "\n",
    "    cost_function = lambda p: cost_neural_network(p, theta_size, Xtrain, ytrain)\n",
    "\n",
    "    # =============== Your code here ============================\n",
    "    # Chose a minimizer function from scipy in order to get the optimal theta\n",
    "    \n",
    "    opt_theta = ...\n",
    "    \n",
    "    # ========================================================\n",
    "\n",
    "    ypredtrain[i] = np.mean(predict_neural_network(opt_theta, theta_size, Xtrain) == ytrain) * 100\n",
    "    ypredval[i] = np.mean(predict_neural_network(opt_theta, theta_size, Xval) == yval) * 100\n",
    "    ypredtest[i] = np.mean(predict_neural_network(opt_theta, theta_size, Xtest) == ytest) * 100\n",
    "\n",
    "\n",
    "# =============== Your code here ============================\n",
    "# Plot the results of ypredtrain and ypredval\n",
    "\n",
    "# ========================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6 - Implement Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-24T13:09:44.689462Z"
    }
   },
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    e_z = np.exp(z - np.max(z, axis=0, keepdims=True))\n",
    "    h = e_z / e_z.sum(axis=0, keepdims=True)\n",
    "    return h\n",
    "\n",
    "def check_gradient(J, theta):\n",
    "    numgrad = np.zeros_like(theta)\n",
    "    epsilon = 1e-4\n",
    "\n",
    "    for i in range(len(numgrad)):\n",
    "        e = np.zeros_like(theta).flatten()\n",
    "        e[i] = epsilon\n",
    "        numgrad[i] = (J(theta + e.reshape(theta.shape)) - J(theta - e.reshape(theta.shape))) / (2 * epsilon)\n",
    "\n",
    "    return numgrad.flatten()\n",
    "\n",
    "def init_softmax_parameters(numin, numout):\n",
    "    theta = np.random.randn(numout, numin)\n",
    "    theta = theta.ravel()\n",
    "    return theta\n",
    "\n",
    "def cost_softmax(theta, X, y, numClasses, lambda_=0):\n",
    "    m, n = X.shape\n",
    "    lambda_ = 0\n",
    "    theta = np.reshape(theta, (numClasses, X.shape[0]))    \n",
    "    y_mat = np.eye(num_classes)[y.reshape(-1)-1].T\n",
    "    \n",
    "    # =============== Your code here ============================\n",
    "    # Calculate the hypothesis h using theta, X, and the function softmax. Use h to calculate J and grad. \n",
    "    # Remember to .ravel() the grad just as we did for the cost_neural_network\n",
    "    \n",
    "    # =============================================================\n",
    "    \n",
    "    return J, grad\n",
    "\n",
    "def predict_softmax(theta, X, numClasses):\n",
    "    \n",
    "    # =============== Your code here ============================\n",
    "    # Use similar as predict_neural_network\n",
    "    \n",
    "    # =============================================================\n",
    "    return preds\n",
    "\n",
    "\n",
    "# Create test data to check gradients\n",
    "num_classes = 10\n",
    "num_inputs = 5\n",
    "num_samples = 20\n",
    "X = np.random.randn(num_inputs, num_samples)\n",
    "y = np.random.randint(1, num_classes+1, size=(1, num_samples))\n",
    "print(X.shape,y.shape)\n",
    "\n",
    "theta = init_softmax_parameters(num_inputs, num_classes)\n",
    "cost, grad = cost_softmax(theta, X, y[0], num_classes)  # Adjust indexing here since y[0] gives the 1D array\n",
    "numGrad = check_gradient(lambda p: cost_softmax(p, X, y[0], num_classes)[0], theta)  # Adjust indexing here too\n",
    "diff = np.linalg.norm(numGrad - grad) / np.linalg.norm(numGrad + grad)\n",
    "print(diff)\n",
    "\n",
    "\n",
    "# =============== Your code here ============================\n",
    "# Use the softmax on the raw MNISTsmall dataset\n",
    "\n",
    "# =============================================================\n",
    "\n",
    "\n",
    "# =============== Your code here ============================\n",
    "# Use the softmax on the trained auto-encoder hidden layer\n",
    "\n",
    "# =============================================================\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7 - PyTorch MNIST Classification Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 401
    },
    "id": "ujMUeHrPw2GW",
    "outputId": "7f42334a-c37c-47d1-ab7f-bc9e93c60bf8",
    "ExecuteTime": {
     "start_time": "2023-11-24T13:09:44.690381Z"
    }
   },
   "outputs": [],
   "source": [
    "# PyTorch MNIST Classification Tutorial\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# We apply a transform to normalize the data.\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Download and load the training data\n",
    "trainset = torchvision.datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Download and load the test data\n",
    "testset = torchvision.datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Create your own neural network now\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # =============== Your code here ============================\n",
    "        # Define your layers here with nn.Linear (optional: Try nn.Conv2d for a CNN)\n",
    "        \n",
    "        # ========================================================\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28) # Flatten the image\n",
    "        \n",
    "        # =============== Your code here ==========================\n",
    "        # Choose the activations functions here for example torch.sigmoid or torch.relu \n",
    "        # and use the previously defined layers above\n",
    "\n",
    "        # ========================================================\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "\n",
    "#  Choose your loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "epochs = 15 #choose a number of epochs\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total}%')\n",
    "# Optional: Visualize some predictions\n",
    "# This will display images along with predicted and true labels.\n",
    "import numpy as np\n",
    "dataiter = iter(testloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# Function to show an image\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5  # Unnormalize the image\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))  # Convert from Tensor image\n",
    "\n",
    "# Show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print('GroundTruth: ', ' '.join(f'{labels[j].item()}' for j in range(10)))\n",
    "\n",
    "# Get predictions\n",
    "outputs = net(images.view(-1, 28*28))  # Reshape images to match the input shape of the network\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "print('  Predicted: ', ' '.join(f'{predicted[j].item()}'\n",
    "                              for j in range(10)))\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
