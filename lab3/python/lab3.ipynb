{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Machine Learning </center>\n",
    "## <center> Lab 3 </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises for Pass (G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Neural Network implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uYF6zY4QTqY5",
    "outputId": "38840d62-8fe5-4ea9-d0e7-03e3ffb0f15b",
    "ExecuteTime": {
     "end_time": "2023-11-24T13:09:44.155308Z",
     "start_time": "2023-11-24T13:09:44.114805Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 100) (1, 100)\n",
      "[(5, 8), (10, 5), (5, 1), (10, 1)]\n",
      "3.6167921578287413e-10\n"
     ]
    }
   ],
   "source": [
    "# Time to implement a 1-layered non-regularized neural network.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_gradient(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "def check_gradient(J, theta):\n",
    "    numgrad = np.zeros_like(theta)\n",
    "    epsilon = 1e-4\n",
    "\n",
    "    for i in range(len(numgrad)):\n",
    "        e = np.zeros_like(theta).flatten()\n",
    "        e[i] = epsilon\n",
    "        numgrad[i] = (J(theta + e.reshape(theta.shape)) - J(theta - e.reshape(theta.shape))) / (2 * epsilon)\n",
    "\n",
    "    return numgrad.flatten()\n",
    "\n",
    "def init_nn_parameters(numin, numhid, numout):\n",
    "    r1  = np.sqrt(6) / np.sqrt(numin+numhid+1)\n",
    "    r2  = np.sqrt(6) / np.sqrt(numin+numout+1)\n",
    "    W1 = np.random.rand(numhid, numin) * 2 * r1 - r1\n",
    "    W2 = np.random.rand(numout, numhid) * 2 * r2 - r2\n",
    "    b1 = np.zeros((numhid, 1))\n",
    "    b2 = np.zeros((numout, 1))\n",
    "    theta = np.concatenate([W1.ravel(), W2.ravel(), b1.ravel(), b2.ravel()])\n",
    "    theta_size = [W1.shape, W2.shape, b1.shape, b2.shape]\n",
    "    return theta, theta_size\n",
    "\n",
    "def theta_to_params(theta, theta_size):\n",
    "    idx_W1_end = np.prod(theta_size[0])\n",
    "    idx_W2_end = idx_W1_end + np.prod(theta_size[1])\n",
    "    idx_b1_end = idx_W2_end + np.prod(theta_size[2])\n",
    "    W1 = theta[:idx_W1_end].reshape(theta_size[0])\n",
    "    W2 = theta[idx_W1_end:idx_W2_end].reshape(theta_size[1])\n",
    "    b1 = theta[idx_W2_end:idx_b1_end].reshape(theta_size[2])\n",
    "    b2 = theta[idx_b1_end:].reshape(theta_size[3])\n",
    "    return W1, W2, b1, b2\n",
    "\n",
    "def cost_neural_network(theta, theta_size, X, y):\n",
    "    m, n = X.shape\n",
    "    num_classes = len(np.unique(y))\n",
    "    lambda_ = 0\n",
    "\n",
    "    W1, W2, b1, b2 = theta_to_params(theta, theta_size) \n",
    "    y_mat = np.eye(num_classes)[y.reshape(-1)-1].T\n",
    "    \n",
    "    # =============== Your code here ============================\n",
    "    # Forward propagation\n",
    "    z2 = np.dot(W1, X) + b1\n",
    "    a2 = sigmoid(z2)\n",
    "    z3 = np.dot(W2, a2) + b2\n",
    "    a3 = sigmoid(z3)\n",
    "\n",
    "    # Calculate cost J\n",
    "    J = np.sum(0.5 * np.sum((a3 - y_mat) ** 2))\n",
    "\n",
    "    # Calculate delta error terms\n",
    "    delta3 = (a3 - y_mat) * a3 * (1 - a3)\n",
    "    delta2 = np.dot(W2.T, delta3) * a2 * (1 - a2)\n",
    "\n",
    "    # Calculate gradients\n",
    "    gradW2 = np.dot(delta3, a2.T)\n",
    "    gradW1 = np.dot(delta2, X.T)\n",
    "    gradb2 = np.sum(delta3, axis=1)\n",
    "    gradb1 = np.sum(delta2, axis=1)\n",
    "\n",
    "     # =============================================================\n",
    "    \n",
    "    # Unroll gradients\n",
    "    grad = np.concatenate([gradW1.ravel(), gradW2.ravel(), gradb1.ravel(), gradb2.ravel()])\n",
    "    \n",
    "    return J, grad\n",
    "\n",
    "# Create test data to check gradients\n",
    "X = np.random.randn(8, 100)\n",
    "y = np.random.randint(1, 11, size=(1, 100))\n",
    "print(X.shape,y.shape)\n",
    "\n",
    "theta, theta_size = init_nn_parameters(8, 5, 10)\n",
    "print(theta_size)\n",
    "cost, grad = cost_neural_network(theta, theta_size, X, y[0])  # Adjust indexing here since y[0] gives the 1D array\n",
    "numGrad = check_gradient(lambda p: cost_neural_network(p, theta_size, X, y[0])[0], theta)  # Adjust indexing here too\n",
    "diff = np.linalg.norm(numGrad - grad) / np.linalg.norm(numGrad + grad)\n",
    "print(diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Neural network for handwritten digit classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ofFQfRvVdMV4",
    "outputId": "71b0bf70-621f-4809-c009-6a7371357f64",
    "ExecuteTime": {
     "end_time": "2023-11-24T13:09:44.687535Z",
     "start_time": "2023-11-24T13:09:44.146610Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './../datasets/smallMNIST.mat'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "File \u001B[0;32m~/PycharmProjects/MLLab1/venv/lib/python3.9/site-packages/scipy/io/matlab/_mio.py:39\u001B[0m, in \u001B[0;36m_open_file\u001B[0;34m(file_like, appendmat, mode)\u001B[0m\n\u001B[1;32m     38\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 39\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mfile_like\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m     40\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     41\u001B[0m     \u001B[38;5;66;03m# Probably \"not found\"\u001B[39;00m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: './../datasets/smallMNIST.mat'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 44\u001B[0m\n\u001B[1;32m     41\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m preds\n\u001B[1;32m     43\u001B[0m \u001B[38;5;66;03m# Load and split data\u001B[39;00m\n\u001B[0;32m---> 44\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[43mscipy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mio\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloadmat\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43mr\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m./../datasets/smallMNIST.mat\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     45\u001B[0m a\u001B[38;5;241m=\u001B[39mdata[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mX\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28mprint\u001B[39m(a\u001B[38;5;241m.\u001B[39mshape)\n",
      "File \u001B[0;32m~/PycharmProjects/MLLab1/venv/lib/python3.9/site-packages/scipy/io/matlab/_mio.py:225\u001B[0m, in \u001B[0;36mloadmat\u001B[0;34m(file_name, mdict, appendmat, **kwargs)\u001B[0m\n\u001B[1;32m     88\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     89\u001B[0m \u001B[38;5;124;03mLoad MATLAB file.\u001B[39;00m\n\u001B[1;32m     90\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    222\u001B[0m \u001B[38;5;124;03m    3.14159265+3.14159265j])\u001B[39;00m\n\u001B[1;32m    223\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    224\u001B[0m variable_names \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvariable_names\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m--> 225\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m _open_file_context(file_name, appendmat) \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[1;32m    226\u001B[0m     MR, _ \u001B[38;5;241m=\u001B[39m mat_reader_factory(f, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    227\u001B[0m     matfile_dict \u001B[38;5;241m=\u001B[39m MR\u001B[38;5;241m.\u001B[39mget_variables(variable_names)\n",
      "File \u001B[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/contextlib.py:117\u001B[0m, in \u001B[0;36m_GeneratorContextManager.__enter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    115\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkwds, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfunc\n\u001B[1;32m    116\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 117\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgen\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    118\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n\u001B[1;32m    119\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgenerator didn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt yield\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/MLLab1/venv/lib/python3.9/site-packages/scipy/io/matlab/_mio.py:17\u001B[0m, in \u001B[0;36m_open_file_context\u001B[0;34m(file_like, appendmat, mode)\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;129m@contextmanager\u001B[39m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_open_file_context\u001B[39m(file_like, appendmat, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[0;32m---> 17\u001B[0m     f, opened \u001B[38;5;241m=\u001B[39m \u001B[43m_open_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile_like\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mappendmat\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     18\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     19\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m f\n",
      "File \u001B[0;32m~/PycharmProjects/MLLab1/venv/lib/python3.9/site-packages/scipy/io/matlab/_mio.py:45\u001B[0m, in \u001B[0;36m_open_file\u001B[0;34m(file_like, appendmat, mode)\u001B[0m\n\u001B[1;32m     43\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m appendmat \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m file_like\u001B[38;5;241m.\u001B[39mendswith(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.mat\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[1;32m     44\u001B[0m         file_like \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.mat\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m---> 45\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mfile_like\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     47\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m(\n\u001B[1;32m     48\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mReader needs file name or open file-like object\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m     49\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: './../datasets/smallMNIST.mat'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from scipy.optimize import minimize\n",
    "import scipy.io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def theta_to_params(theta, theta_size):\n",
    "    idx_W1_end = np.prod(theta_size[0])\n",
    "    idx_W2_end = idx_W1_end + np.prod(theta_size[1])\n",
    "    idx_b1_end = idx_W2_end + np.prod(theta_size[2])\n",
    "    W1 = theta[:idx_W1_end].reshape(theta_size[0])\n",
    "    W2 = theta[idx_W1_end:idx_W2_end].reshape(theta_size[1])\n",
    "    b1 = theta[idx_W2_end:idx_b1_end].reshape(theta_size[2])\n",
    "    b2 = theta[idx_b1_end:].reshape(theta_size[3])\n",
    "    return W1, W2, b1, b2\n",
    "\n",
    "def split_data(X, ratios, seed):\n",
    "    np.random.seed(seed)\n",
    "    m = X.shape[1]\n",
    "    indices = np.random.permutation(m)\n",
    "    train_end = int(ratios[0] * m)\n",
    "    val_end = train_end + int(ratios[1] * m)\n",
    "    train_idx = indices[:train_end]\n",
    "    val_idx = indices[train_end:val_end]\n",
    "    test_idx = indices[val_end:]\n",
    "    return X[:, train_idx], X[:, val_idx], X[:, test_idx]\n",
    "\n",
    "def predict_neural_network(theta, theta_size, X):\n",
    "    \n",
    "    W1, W2, b1, b2 = theta_to_params(theta, theta_size) \n",
    "\n",
    "    # =============== Your code here ============================\n",
    "    # Calculate the feedforward of the hidden layer and output layer. \n",
    "    # Remember to use the sigmoid function. The output preds is a vector of length m\n",
    "    # with the predicted class (1-10) for each input example where the predicted\n",
    "    # class is the argmax of the output layer\n",
    "    \n",
    "    \n",
    "    # =============================================================\n",
    "    return preds\n",
    "\n",
    "# Load and split data\n",
    "data = scipy.io.loadmat(r'./../datasets/smallMNIST.mat')\n",
    "a=data['X']\n",
    "print(a.shape)\n",
    "X = data['X'].T\n",
    "y = data['y'].reshape(-1, 1).T\n",
    "print(np.unique(y)) # Note that the label vector y starts from 1\n",
    "Xtrain, Xval, Xtest = split_data(X, [0.6, 0.3, 0.1], 0)\n",
    "ytrain, yval, ytest = split_data(y, [0.6, 0.3, 0.1], 0)\n",
    "\n",
    "# Initialize the model\n",
    "num_vis = X.shape[0]\n",
    "num_out = len(np.unique(y))\n",
    "theta, theta_size = init_nn_parameters(num_vis, 50, num_out)\n",
    "\n",
    "# Train the model\n",
    "cost_function = lambda p: cost_neural_network(p, theta_size, Xtrain, ytrain)\n",
    "result = minimize(fun=cost_function, x0=theta, jac=True, method='L-BFGS-B', options={'maxiter': 100, 'disp': True})\n",
    "optTheta = result.x\n",
    "\n",
    "# Calculate the predictions of the trained model\n",
    "ypredtrain = predict_neural_network(optTheta, theta_size, Xtrain)\n",
    "ypredval = predict_neural_network(optTheta, theta_size, Xval)\n",
    "ypredtest = predict_neural_network(optTheta, theta_size, Xtest)\n",
    "\n",
    "# and the accuracies\n",
    "print('Train Set Accuracy:', np.mean(ypredtrain == ytrain) * 100)\n",
    "print('Val Set Accuracy:', np.mean(ypredval == yval) * 100)\n",
    "print('Test Set Accuracy:', np.mean(ypredtest == ytest) * 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Implement Auto-encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oNfRsN8oOwks",
    "outputId": "f3456344-7f37-4a26-a991-bac4dac18d08",
    "ExecuteTime": {
     "start_time": "2023-11-24T13:09:44.685302Z"
    }
   },
   "outputs": [],
   "source": [
    "# Time to implement a non-regularized auto-encoder.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_gradient(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "def theta_to_params(theta, theta_size):\n",
    "    idx_W1_end = np.prod(theta_size[0])\n",
    "    idx_W2_end = idx_W1_end + np.prod(theta_size[1])\n",
    "    idx_b1_end = idx_W2_end + np.prod(theta_size[2])\n",
    "    W1 = theta[:idx_W1_end].reshape(theta_size[0])\n",
    "    W2 = theta[idx_W1_end:idx_W2_end].reshape(theta_size[1])\n",
    "    b1 = theta[idx_W2_end:idx_b1_end].reshape(theta_size[2])\n",
    "    b2 = theta[idx_b1_end:].reshape(theta_size[3])\n",
    "    return W1, W2, b1, b2\n",
    "\n",
    "def cost_autoencoder(theta, theta_size, X):\n",
    "    m, n = X.shape\n",
    "\n",
    "    W1, W2, b1, b2 = theta_to_params(theta, theta_size)  \n",
    "    \n",
    "    # =============== Your code here ============================\n",
    "    # You can copy the code from cost_neural_network and work from there\n",
    "    \n",
    "    # ============================================================\n",
    "    \n",
    "    # Unroll gradients\n",
    "    grad = np.concatenate([gradW1.ravel(), gradW2.ravel(), gradb1.ravel(), gradb2.ravel()])\n",
    "    \n",
    "    return J, grad\n",
    "\n",
    "# Usage for autoencoder\n",
    "theta, theta_size = init_nn_parameters(8, 5, 8)  # Notice the third parameter is 8 for the autoencoder\n",
    "X = np.random.randn(8, 100)  # Input data\n",
    "\n",
    "# Initialize theta and theta_size appropriately before this\n",
    "cost, grad = cost_autoencoder(theta, theta_size, X)\n",
    "\n",
    "# Perform gradient checking as before\n",
    "numGrad = check_gradient(lambda p: cost_autoencoder(p, theta_size, X)[0], theta)\n",
    "\n",
    "# Compute the relative difference between the gradients\n",
    "diff = np.linalg.norm(numGrad - grad) / np.linalg.norm(numGrad + grad)\n",
    "\n",
    "print(\"Gradient difference:\", diff)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 - Reconsructing with Auto-encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "gd-44zQJwMcn",
    "outputId": "7b33935f-53bb-4a9b-d855-28828ca64626",
    "ExecuteTime": {
     "start_time": "2023-11-24T13:09:44.686930Z"
    }
   },
   "outputs": [],
   "source": [
    "def theta_to_params(theta, theta_size):\n",
    "    idx_W1_end = np.prod(theta_size[0])\n",
    "    idx_W2_end = idx_W1_end + np.prod(theta_size[1])\n",
    "    idx_b1_end = idx_W2_end + np.prod(theta_size[2])\n",
    "    W1 = theta[:idx_W1_end].reshape(theta_size[0])\n",
    "    W2 = theta[idx_W1_end:idx_W2_end].reshape(theta_size[1])\n",
    "    b1 = theta[idx_W2_end:idx_b1_end].reshape(theta_size[2])\n",
    "    b2 = theta[idx_b1_end:].reshape(theta_size[3])\n",
    "    return W1, W2, b1, b2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = scipy.io.loadmat(r'./../datasets/smallMNIST.mat')\n",
    "X = data['X']\n",
    "print(\"x shape\",X.shape)\n",
    "\n",
    "Xtrain, Xtemp = train_test_split(X, test_size=0.4, random_state=0)\n",
    "Xval, Xtest = train_test_split(Xtemp, test_size=0.5, random_state=0)\n",
    "print(\"Xtrain shape, Xtemp shape \", Xtrain.shape,Xtemp.shape)\n",
    "\n",
    "# Decide a value for number of hidden units in the auto-encoder, num_hid,\n",
    "# and the number of training iterations, num_iter. \n",
    "# Try different values until you get good results\n",
    "num_hid = 10\n",
    "num_iter = 10\n",
    "\n",
    "num_vis = Xtrain.shape[1]\n",
    "theta, theta_size = init_nn_parameters(num_vis, num_hid, num_vis)\n",
    "Xtrain = Xtrain.T\n",
    "print(\"Xtrain.T shape\", Xtrain.shape)\n",
    "\n",
    "# Train the auto-encoder\n",
    "cost_function = lambda p: cost_autoencoder(p, theta_size, Xtrain)\n",
    "# use an optimizing function to get the optimal thetas\n",
    "result = minimize(fun=cost_function, x0=theta, method='L-BFGS-B', jac=True, options={'maxiter': num_iter, 'disp': True})\n",
    "opt_theta = result.x\n",
    "\n",
    "# =============== Your code here ============================\n",
    "# Feedforward the training data on the trained auto-encoder to get the output layer Xrec.\n",
    "# You can use the function theta_to_params to get W1, W2, b1, b2 from opt_theta and theta_size\n",
    "\n",
    "Xrec = ...\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "# Plot 10x10 images from the original data and the reconstructed data\n",
    "def display_data(data, title):\n",
    "    fig, axarr = plt.subplots(10, 10, figsize=(10, 10))\n",
    "    for i in range(10):\n",
    "        for j in range(10):\n",
    "            axarr[i, j].imshow(data[:, i * 10 + j].reshape(20, 20).T, cmap='gray')\n",
    "            axarr[i, j].axis('off')\n",
    "    plt.suptitle(title)\n",
    "    plt.show()\n",
    "\n",
    "# Display original images\n",
    "print(\"Xtrain.T shape emd\", Xtrain.shape)\n",
    "display_data(Xtrain, 'Original input')\n",
    "\n",
    "# Display reconstructed images\n",
    "display_data(Xrec, 'Reconstructions')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises for Pass with distinction (VG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5 - Bias-variance analysis on the number of hidden units in a Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YjcR7eLuMNxK",
    "outputId": "b2929695-170e-4611-d722-c1fabbd41330",
    "ExecuteTime": {
     "start_time": "2023-11-24T13:09:44.688489Z"
    }
   },
   "outputs": [],
   "source": [
    "# Perform a bias-variance analysis on the number of\n",
    "# hidden units in the neural network (numhid) to decide the optimal value. Show the\n",
    "# bias-variance analysis plot and describe what is the best choice.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# choose 10 different number for your hidden layer and decide the optimal value throught the plot\n",
    "numhidlist = [1,2,3,4,5,6,7,8,9,10] # You can change these values to try different values of numhid\n",
    "\n",
    "ypredtrain = np.zeros(len(numhidlist))\n",
    "ypredval = np.zeros(len(numhidlist))\n",
    "ypredtest = np.zeros(len(numhidlist))\n",
    "\n",
    "# Load and split data\n",
    "data = scipy.io.loadmat(r'./../datasets/smallMNIST.mat')\n",
    "X = data['X'].T\n",
    "y = data['y'].reshape(-1, 1).T\n",
    "Xtrain, Xval, Xtest = split_data(X, [0.6, 0.3, 0.1], 0)\n",
    "ytrain, yval, ytest = split_data(y, [0.6, 0.3, 0.1], 0)\n",
    "\n",
    "num_vis = Xtrain.shape[0]\n",
    "num_out = len(np.unique(ytrain))\n",
    "\n",
    "\n",
    "for i, numhid in enumerate(numhidlist):\n",
    "    theta, theta_size = init_nn_parameters(num_vis, numhid, num_out)\n",
    "\n",
    "    cost_function = lambda p: cost_neural_network(p, theta_size, Xtrain, ytrain)\n",
    "\n",
    "    # =============== Your code here ============================\n",
    "    # Chose a minimizer function from scipy in order to get the optimal theta\n",
    "    \n",
    "    opt_theta = ...\n",
    "    \n",
    "    # ========================================================\n",
    "\n",
    "    ypredtrain[i] = np.mean(predict_neural_network(opt_theta, theta_size, Xtrain) == ytrain) * 100\n",
    "    ypredval[i] = np.mean(predict_neural_network(opt_theta, theta_size, Xval) == yval) * 100\n",
    "    ypredtest[i] = np.mean(predict_neural_network(opt_theta, theta_size, Xtest) == ytest) * 100\n",
    "\n",
    "\n",
    "# =============== Your code here ============================\n",
    "# Plot the results of ypredtrain and ypredval\n",
    "\n",
    "# ========================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6 - Implement Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-24T13:09:44.689462Z"
    }
   },
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    e_z = np.exp(z - np.max(z, axis=0, keepdims=True))\n",
    "    h = e_z / e_z.sum(axis=0, keepdims=True)\n",
    "    return h\n",
    "\n",
    "def check_gradient(J, theta):\n",
    "    numgrad = np.zeros_like(theta)\n",
    "    epsilon = 1e-4\n",
    "\n",
    "    for i in range(len(numgrad)):\n",
    "        e = np.zeros_like(theta).flatten()\n",
    "        e[i] = epsilon\n",
    "        numgrad[i] = (J(theta + e.reshape(theta.shape)) - J(theta - e.reshape(theta.shape))) / (2 * epsilon)\n",
    "\n",
    "    return numgrad.flatten()\n",
    "\n",
    "def init_softmax_parameters(numin, numout):\n",
    "    theta = np.random.randn(numout, numin)\n",
    "    theta = theta.ravel()\n",
    "    return theta\n",
    "\n",
    "def cost_softmax(theta, X, y, numClasses, lambda_=0):\n",
    "    m, n = X.shape\n",
    "    lambda_ = 0\n",
    "    theta = np.reshape(theta, (numClasses, X.shape[0]))    \n",
    "    y_mat = np.eye(num_classes)[y.reshape(-1)-1].T\n",
    "    \n",
    "    # =============== Your code here ============================\n",
    "    # Calculate the hypothesis h using theta, X, and the function softmax. Use h to calculate J and grad. \n",
    "    # Remember to .ravel() the grad just as we did for the cost_neural_network\n",
    "    \n",
    "    # =============================================================\n",
    "    \n",
    "    return J, grad\n",
    "\n",
    "def predict_softmax(theta, X, numClasses):\n",
    "    \n",
    "    # =============== Your code here ============================\n",
    "    # Use similar as predict_neural_network\n",
    "    \n",
    "    # =============================================================\n",
    "    return preds\n",
    "\n",
    "\n",
    "# Create test data to check gradients\n",
    "num_classes = 10\n",
    "num_inputs = 5\n",
    "num_samples = 20\n",
    "X = np.random.randn(num_inputs, num_samples)\n",
    "y = np.random.randint(1, num_classes+1, size=(1, num_samples))\n",
    "print(X.shape,y.shape)\n",
    "\n",
    "theta = init_softmax_parameters(num_inputs, num_classes)\n",
    "cost, grad = cost_softmax(theta, X, y[0], num_classes)  # Adjust indexing here since y[0] gives the 1D array\n",
    "numGrad = check_gradient(lambda p: cost_softmax(p, X, y[0], num_classes)[0], theta)  # Adjust indexing here too\n",
    "diff = np.linalg.norm(numGrad - grad) / np.linalg.norm(numGrad + grad)\n",
    "print(diff)\n",
    "\n",
    "\n",
    "# =============== Your code here ============================\n",
    "# Use the softmax on the raw MNISTsmall dataset\n",
    "\n",
    "# =============================================================\n",
    "\n",
    "\n",
    "# =============== Your code here ============================\n",
    "# Use the softmax on the trained auto-encoder hidden layer\n",
    "\n",
    "# =============================================================\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7 - PyTorch MNIST Classification Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 401
    },
    "id": "ujMUeHrPw2GW",
    "outputId": "7f42334a-c37c-47d1-ab7f-bc9e93c60bf8",
    "ExecuteTime": {
     "start_time": "2023-11-24T13:09:44.690381Z"
    }
   },
   "outputs": [],
   "source": [
    "# PyTorch MNIST Classification Tutorial\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# We apply a transform to normalize the data.\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Download and load the training data\n",
    "trainset = torchvision.datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Download and load the test data\n",
    "testset = torchvision.datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Create your own neural network now\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # =============== Your code here ============================\n",
    "        # Define your layers here with nn.Linear (optional: Try nn.Conv2d for a CNN)\n",
    "        \n",
    "        # ========================================================\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28) # Flatten the image\n",
    "        \n",
    "        # =============== Your code here ==========================\n",
    "        # Choose the activations functions here for example torch.sigmoid or torch.relu \n",
    "        # and use the previously defined layers above\n",
    "\n",
    "        # ========================================================\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "\n",
    "#  Choose your loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "epochs = 15 #choose a number of epochs\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total}%')\n",
    "# Optional: Visualize some predictions\n",
    "# This will display images along with predicted and true labels.\n",
    "import numpy as np\n",
    "dataiter = iter(testloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# Function to show an image\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5  # Unnormalize the image\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))  # Convert from Tensor image\n",
    "\n",
    "# Show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print('GroundTruth: ', ' '.join(f'{labels[j].item()}' for j in range(10)))\n",
    "\n",
    "# Get predictions\n",
    "outputs = net(images.view(-1, 28*28))  # Reshape images to match the input shape of the network\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "print('  Predicted: ', ' '.join(f'{predicted[j].item()}'\n",
    "                              for j in range(10)))\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
